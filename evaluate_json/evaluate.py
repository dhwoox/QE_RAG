#!/usr/bin/env python3
"""
ì•ˆì •í™”ëœ ì„ë² ë”© ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
TestCase íŠ¹í™” í‰ê°€ë¡œ ì—…ë°ì´íŠ¸ (Tech ì§€í‘œ ê°œì„ )
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from collections import defaultdict
from transformers import AutoTokenizer, AutoModel
import time
import warnings
import gc
import torch
warnings.filterwarnings('ignore')

def clear_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def load_sfr_model(model_name):
    """Jina, SFR ëª¨ë¸ ì „ìš© ë¡œë”"""
    try:
        print(f"  ğŸ”„ SFR ëª¨ë¸ ë¡œë”©: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            model = model.cuda()
            print("  ğŸ“± GPUë¡œ ëª¨ë¸ ì´ë™ ì™„ë£Œ")
        
        return model, tokenizer
    except Exception as e:
        print(f"  âŒ SFR ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

def encode_with_sfr(texts, model, tokenizer, batch_size=1):
    """SFR ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± (ëª¨ë¸ë³„ ìµœì í™”)"""
    try:
        import torch.nn.functional as F
        
        # ëª¨ë¸ë³„ ì„¤ì •
        model_name = getattr(model, 'name_or_path', str(model))
        
        if 'Code-2B_R' in model_name:
            # Code-2B_R ëª¨ë¸: encode_corpus ë©”ì„œë“œ ì‚¬ìš©
            print("    ğŸ”§ Code-2B_R ëª¨ë¸ìš© ì¸ì½”ë”©")
            try:
                # Code ëª¨ë¸ì€ íŠ¹ë³„í•œ ì¸ì½”ë”© ë©”ì„œë“œë¥¼ ì œê³µ
                embeddings = model.encode_corpus(texts, max_length=4096)
                if isinstance(embeddings, torch.Tensor):
                    embeddings = embeddings.cpu().numpy()
                return embeddings
            except AttributeError:
                print("    âš ï¸ encode_corpus ë©”ì„œë“œ ì—†ìŒ, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©")
                # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
                pass
        
        # ê¸°ë³¸ SFR ì„ë² ë”© ìƒì„± (Mistral ë“±)
        def last_token_pool(last_hidden_states, attention_mask):
            """Last token pooling"""
            left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
            if left_padding:
                return last_hidden_states[:, -1]
            else:
                sequence_lengths = attention_mask.sum(dim=1) - 1
                batch_size = last_hidden_states.shape[0]
                return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]
        
        embeddings = []
        model.eval()
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # í† í¬ë‚˜ì´ì§•
                max_length = 32768 if 'Code-2B_R' in model_name else 4096
                inputs = tokenizer(batch_texts, 
                                 return_tensors="pt", 
                                 padding=True, 
                                 truncation=True, 
                                 max_length=max_length)
                
                # GPUë¡œ ì´ë™ (ëª¨ë¸ì´ GPUì— ìˆëŠ” ê²½ìš°)
                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}
                
                # ëª¨ë¸ ì¶”ë¡ 
                outputs = model(**inputs)
                
                # Last token pooling ì ìš©
                batch_embeddings = last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])
                
                # ì •ê·œí™”
                batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)
                
                # CPUë¡œ ì´ë™í•˜ê³  numpyë¡œ ë³€í™˜
                batch_embeddings = batch_embeddings.cpu().numpy()
                embeddings.append(batch_embeddings)
                
                # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i % 5 == 0:
                    clear_memory()
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        return np.vstack(embeddings)
        
    except Exception as e:
        print(f"    SFR ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def evaluate_single_model(embedding_list, model_name, max_texts=None, batch_size=2):
    """
    ë‹¨ì¼ ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        embedding_list: ì´ìŠˆë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        model_name: ëª¨ë¸ëª…
        max_texts: ìµœëŒ€ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        batch_size: ë°°ì¹˜ í¬ê¸° (ì‘ê²Œ ì„¤ì •)
    """
    
    print(f"\nğŸ”¬ {model_name} ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_texts = []
    text_to_issue = {}
    
    for issue_idx, issue_text_list in enumerate(embedding_list):
        issue_id = f"issue_{issue_idx}"
        
        if isinstance(issue_text_list, list):
            for text_idx, text in enumerate(issue_text_list):
                if text and isinstance(text, str) and text.strip():
                    all_texts.append(text)
                    text_to_issue[len(all_texts)-1] = f"{issue_id}_text_{text_idx}"
                    
                    # ìµœëŒ€ í…ìŠ¤íŠ¸ ìˆ˜ ì œí•œ
                    if max_texts and len(all_texts) >= max_texts:
                        break
            if max_texts and len(all_texts) >= max_texts:
                break

    if not all_texts:
        print("  âŒ í‰ê°€í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"  ğŸ“ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜ˆì • (ì „ì²´ ì„ë² ë”©)")
    print(f"  ğŸ¯ TestCase í‰ê°€ëŠ” ì²˜ìŒ 3ê°œ ì´ìŠˆë§Œ ì‚¬ìš©")
    
    try:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_memory()
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        print("  ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # SFR ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬
        if 'SFR-Embedding' in model_name or 'sfr-embedding' in model_name.lower() or 'jina-embedding' in model_name:
            print("  ğŸ¯ SFR ëª¨ë¸ ê°ì§€ - íŠ¹ë³„ ì²˜ë¦¬ ëª¨ë“œ")
            sfr_model, sfr_tokenizer = load_sfr_model(model_name)
            
            if sfr_model is None or sfr_tokenizer is None:
                print("  âŒ SFR ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return None
            
            # SFR ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
            print("  ğŸ”„ SFR ì„ë² ë”© ìƒì„± ì¤‘...")
            start_time = time.time()
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
            chunk_size = min(20, len(all_texts))  # SFRì€ 20ê°œì”© ì²˜ë¦¬
            all_embeddings = []
            
            for i in range(0, len(all_texts), chunk_size):
                chunk_texts = all_texts[i:i+chunk_size]
                print(f"    ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+chunk_size, len(all_texts))}/{len(all_texts)}")
                
                chunk_embeddings = encode_with_sfr(chunk_texts, sfr_model, sfr_tokenizer, batch_size=1)
                if chunk_embeddings is not None:
                    all_embeddings.append(chunk_embeddings)
                
                # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i % 20 == 0:
                    clear_memory()
            
            if not all_embeddings:
                print("  âŒ SFR ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return None
            
            # ëª¨ë“  ì„ë² ë”© í•©ì¹˜ê¸°
            embeddings = np.vstack(all_embeddings)
            embedding_time = time.time() - start_time
            
            # SFR ëª¨ë¸ ì •ë¦¬
            del sfr_model
            del sfr_tokenizer
            clear_memory()
            
        else:
            # ê¸°ì¡´ SentenceTransformer ë°©ì‹ (ë‹¤êµ­ì–´ ëª¨ë¸ë“¤)
            # ëª¨ë¸ë³„ ë¡œë”© ì „ëµ
            if 'jina' in model_name.lower():
                model = SentenceTransformer(model_name, trust_remote_code=True)
            elif 'bge' in model_name.lower():
                model = SentenceTransformer(model_name)
            elif 'multilingual-e5' in model_name.lower():
                model = SentenceTransformer(model_name)
            elif 'e5-mistral' in model_name.lower():
                model = SentenceTransformer(model_name, trust_remote_code=True)
            else:
                model = SentenceTransformer(model_name, trust_remote_code=True)
                
            print(f"  âœ… ë‹¤êµ­ì–´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            
            # ì„ë² ë”© ìƒì„±
            print("  ğŸ”„ ë‹¤êµ­ì–´ ì„ë² ë”© ìƒì„± ì¤‘...")
            start_time = time.time()
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
            chunk_size = min(50, len(all_texts))  # 50ê°œì”© ì²˜ë¦¬
            all_embeddings = []
            
            for i in range(0, len(all_texts), chunk_size):
                chunk_texts = all_texts[i:i+chunk_size]
                print(f"    ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+chunk_size, len(all_texts))}/{len(all_texts)}")
                
                chunk_embeddings = model.encode(
                    chunk_texts, 
                    batch_size=batch_size,
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True  # ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ì •ê·œí™” ì¤‘ìš”
                )
                all_embeddings.append(chunk_embeddings)
                
                # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i % 100 == 0:
                    clear_memory()
            
            # ëª¨ë“  ì„ë² ë”© í•©ì¹˜ê¸°
            embeddings = np.vstack(all_embeddings)
            embedding_time = time.time() - start_time
            
            # ëª¨ë¸ ì •ë¦¬
            del model
            clear_memory()
        
        print(f"  âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ ({embedding_time:.2f}ì´ˆ)")
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥
        evaluation_results = {
            'model_name': model_name,
            'total_texts': len(all_texts),
            'total_issues': len(embedding_list),
            'embedding_dimension': embeddings.shape[1],
            'embedding_time': embedding_time,
            'texts_per_second': len(all_texts) / embedding_time if embedding_time > 0 else 0,
            'batch_size_used': batch_size
        }
        
        # í’ˆì§ˆ í‰ê°€ (ì „ì²´ ë°ì´í„°)
        print("  ğŸ“Š í’ˆì§ˆ í‰ê°€ ì¤‘...")
        quality_metrics = evaluate_embedding_quality(embeddings)
        evaluation_results.update(quality_metrics)
        
        # ìœ ì‚¬ë„ ë¶„ì„ (ì „ì²´ ë°ì´í„°)
        print("  ğŸ” ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...")
        similarity_metrics = analyze_similarity_distribution(embeddings, text_to_issue)
        evaluation_results.update(similarity_metrics)
        
        # í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ (ì „ì²´ ë°ì´í„°)
        print("  ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ ì¤‘...")
        clustering_metrics = evaluate_clustering_performance(embeddings, text_to_issue)
        evaluation_results.update(clustering_metrics)
        
        # TestCase íŠ¹í™” í‰ê°€ ì¶”ê°€ (ì²˜ìŒ 3ê°œ ì´ìŠˆë§Œ ì‚¬ìš©)
        print("  ğŸ¯ TestCase íŠ¹í™” í‰ê°€ ì¤‘...")
        testcase_metrics = evaluate_testcase_performance(embeddings, text_to_issue, all_texts, embedding_list)
        evaluation_results.update(testcase_metrics)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del embeddings
        clear_memory()
        
        print(f"  âœ… {model_name} í‰ê°€ ì™„ë£Œ!")
        return evaluation_results
        
    except Exception as e:
        print(f"  âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        clear_memory()
        return None

def evaluate_embedding_quality(embeddings):
    """ì„ë² ë”© í’ˆì§ˆ í‰ê°€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    try:
        # ë²¡í„° í¬ê¸° ë¶„í¬
        norms = np.linalg.norm(embeddings, axis=1)
        
        # ì°¨ì›ë³„ ë¶„ì‚° (ìƒ˜í”Œë§)
        if embeddings.shape[1] > 1000:
            # ì°¨ì›ì´ ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§
            sample_dims = np.random.choice(embeddings.shape[1], 1000, replace=False)
            dim_variances = np.var(embeddings[:, sample_dims], axis=0)
        else:
            dim_variances = np.var(embeddings, axis=0)
        
        # ì„ë² ë”© ê°„ í‰ê·  ê±°ë¦¬ (ìƒ˜í”Œë§)
        if len(embeddings) > 100:
            # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§
            sample_indices = np.random.choice(len(embeddings), 100, replace=False)
            sample_embeddings = embeddings[sample_indices]
            distances = euclidean_distances(sample_embeddings)
        else:
            distances = euclidean_distances(embeddings)
            
        avg_distance = np.mean(distances[np.triu_indices_from(distances, k=1)])
        
        return {
            'vector_norm_mean': float(np.mean(norms)),
            'vector_norm_std': float(np.std(norms)),
            'dimension_variance_mean': float(np.mean(dim_variances)),
            'dimension_variance_std': float(np.std(dim_variances)),
            'average_euclidean_distance': float(avg_distance),
            'embedding_density': float(1.0 / avg_distance if avg_distance > 0 else 0)
        }
    except Exception as e:
        print(f"    í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
        return {
            'vector_norm_mean': 0.0, 'vector_norm_std': 0.0,
            'dimension_variance_mean': 0.0, 'dimension_variance_std': 0.0,
            'average_euclidean_distance': 0.0, 'embedding_density': 0.0
        }

def analyze_similarity_distribution(embeddings, text_to_issue):
    """ìœ ì‚¬ë„ ë¶„ì„ (ê°œì„ ëœ ë””ë²„ê¹… ë²„ì „)"""
    try:
        print(f"    ğŸ” ìœ ì‚¬ë„ ë¶„ì„ ë””ë²„ê¹…: ì„ë² ë”© shape = {embeddings.shape}")
        
        # ì„ë² ë”© ì •ê·œí™”
        from sklearn.preprocessing import normalize
        normalized_embeddings = normalize(embeddings, norm='l2')
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì²­í¬ ë‹¨ìœ„)
        if len(embeddings) > 100:
            # ìƒ˜í”Œë§
            sample_indices = np.random.choice(len(embeddings), 100, replace=False)
            sample_embeddings = normalized_embeddings[sample_indices]
            sample_text_to_issue = {i: text_to_issue[sample_indices[i]] for i in range(len(sample_indices))}
            similarities = cosine_similarity(sample_embeddings)
            current_text_to_issue = sample_text_to_issue
        else:
            similarities = cosine_similarity(normalized_embeddings)
            current_text_to_issue = text_to_issue
        
        print(f"    ğŸ“Š ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {similarities.shape}")
        print(f"    ğŸ“ˆ ìœ ì‚¬ë„ ë²”ìœ„: {similarities.min():.4f} ~ {similarities.max():.4f}")
        
        # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸
        upper_triangle = similarities[np.triu_indices_from(similarities, k=1)]
        
        # ê°™ì€ ì´ìŠˆ ë‚´ í…ìŠ¤íŠ¸ë“¤ ê°„ ìœ ì‚¬ë„
        same_issue_similarities = []
        different_issue_similarities = []
        
        issue_pair_count = defaultdict(int)
        
        for i in range(len(similarities)):
            for j in range(i+1, len(similarities)):
                sim = similarities[i][j]
                issue_i = current_text_to_issue[i].split('_')[0]
                issue_j = current_text_to_issue[j].split('_')[0]
                
                if issue_i == issue_j:
                    same_issue_similarities.append(sim)
                    issue_pair_count['same'] += 1
                else:
                    different_issue_similarities.append(sim)
                    issue_pair_count['different'] += 1
        
        print(f"    ğŸ”— ê°™ì€ ì´ìŠˆ ë‚´ ìŒ: {issue_pair_count['same']}ê°œ")
        print(f"    ğŸ”— ë‹¤ë¥¸ ì´ìŠˆ ê°„ ìŒ: {issue_pair_count['different']}ê°œ")
        
        # ì•ˆì „í•œ í‰ê·  ê³„ì‚°
        same_issue_mean = float(np.mean(same_issue_similarities)) if same_issue_similarities else 0.0
        different_issue_mean = float(np.mean(different_issue_similarities)) if different_issue_similarities else 0.0
        ratio = same_issue_mean / different_issue_mean if different_issue_mean > 0 else 0.0
        
        print(f"    ğŸ“Š ê°™ì€ ì´ìŠˆ ë‚´ í‰ê·  ìœ ì‚¬ë„: {same_issue_mean:.4f}")
        print(f"    ğŸ“Š ë‹¤ë¥¸ ì´ìŠˆ ê°„ í‰ê·  ìœ ì‚¬ë„: {different_issue_mean:.4f}")
        print(f"    ğŸ“Š ìœ ì‚¬ë„ ë¹„ìœ¨: {ratio:.4f}")
        
        # ì„ë² ë”© ë¶„í¬ í™•ì¸
        embedding_norms = np.linalg.norm(embeddings, axis=1)
        print(f"    ğŸ“ ì„ë² ë”© í¬ê¸° ë¶„í¬: {embedding_norms.min():.4f} ~ {embedding_norms.max():.4f} (í‰ê· : {embedding_norms.mean():.4f})")
        
        return {
            'similarity_mean': float(np.mean(upper_triangle)),
            'similarity_std': float(np.std(upper_triangle)),
            'similarity_min': float(np.min(upper_triangle)),
            'similarity_max': float(np.max(upper_triangle)),
            'same_issue_similarity_mean': same_issue_mean,
            'different_issue_similarity_mean': different_issue_mean,
            'intra_vs_inter_ratio': ratio,
            'same_issue_pairs': issue_pair_count.get('same', 0),
            'different_issue_pairs': issue_pair_count.get('different', 0),
            'embedding_norm_mean': float(embedding_norms.mean()),
            'embedding_norm_std': float(embedding_norms.std())
        }
    except Exception as e:
        print(f"    âŒ ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {
            'similarity_mean': 0.0, 'similarity_std': 0.0,
            'similarity_min': 0.0, 'similarity_max': 0.0,
            'same_issue_similarity_mean': 0.0, 'different_issue_similarity_mean': 0.0,
            'intra_vs_inter_ratio': 0.0, 'same_issue_pairs': 0, 'different_issue_pairs': 0,
            'embedding_norm_mean': 0.0, 'embedding_norm_std': 0.0
        }

def evaluate_clustering_performance(embeddings, text_to_issue):
    """í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í‰ê°€ (ê°œì„ ëœ ë²„ì „)"""
    try:
        print(f"    ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ë””ë²„ê¹…: ì„ë² ë”© shape = {embeddings.shape}")
        
        # ì´ìŠˆë³„ë¡œ ê·¸ë£¹í™”
        issue_groups = defaultdict(list)
        for idx, issue_info in text_to_issue.items():
            issue_id = issue_info.split('_')[0]
            issue_groups[issue_id].append(idx)
        
        num_issues = len(issue_groups)
        print(f"    ğŸ“Š ì´ìŠˆ ìˆ˜: {num_issues}, í…ìŠ¤íŠ¸ ìˆ˜: {len(embeddings)}")
        
        # ê° ì´ìŠˆë³„ í…ìŠ¤íŠ¸ ê°œìˆ˜ ì¶œë ¥
        for issue_id, indices in issue_groups.items():
            print(f"      - {issue_id}: {len(indices)}ê°œ í…ìŠ¤íŠ¸")
        
        if num_issues < 2 or len(embeddings) < 2:
            print(f"    âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€: ì´ìŠˆ ìˆ˜({num_issues}) ë˜ëŠ” ì„ë² ë”© ìˆ˜({len(embeddings)})ê°€ ë¶€ì¡±")
            return {
                'clustering_score': 0.0, 
                'best_clustering_score': 0.0,
                'optimal_clusters': 0,
                'actual_issues': num_issues,
                'debug_info': f"issues={num_issues}, embeddings={len(embeddings)}"
            }
        
        # ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´)
        from sklearn.preprocessing import normalize
        normalized_embeddings = normalize(embeddings, norm='l2')
        
        # ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
        best_score = -1.0
        best_n_clusters = 2
        
        for n_clusters in range(2, min(num_issues + 1, len(embeddings), 11)):
            try:
                print(f"    ğŸ¯ í´ëŸ¬ìŠ¤í„° ìˆ˜ {n_clusters} í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                kmeans = KMeans(
                    n_clusters=n_clusters, 
                    random_state=42, 
                    n_init=10,
                    max_iter=300
                )
                cluster_labels = kmeans.fit_predict(normalized_embeddings)
                
                # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ í™•ì¸
                unique_clusters = len(set(cluster_labels))
                print(f"      - ì‹¤ì œ ìƒì„±ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {unique_clusters}")
                
                if unique_clusters > 1 and unique_clusters == n_clusters:
                    silhouette = silhouette_score(normalized_embeddings, cluster_labels)
                    print(f"      - ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette:.4f}")
                    
                    if silhouette > best_score:
                        best_score = silhouette
                        best_n_clusters = n_clusters
                else:
                    print(f"      - í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ (ìœ ë‹ˆí¬ í´ëŸ¬ìŠ¤í„°: {unique_clusters})")
                    
            except Exception as cluster_error:
                print(f"      - í´ëŸ¬ìŠ¤í„° {n_clusters} ì˜¤ë¥˜: {cluster_error}")
                continue
        
        # ì¶”ê°€ í‰ê°€: ì´ìŠˆ ê¸°ë°˜ í‰ê°€ (ì‹¤ì œ ë ˆì´ë¸”ê³¼ì˜ ë¹„êµ)
        issue_based_score = 0.0
        try:
            # ì‹¤ì œ ì´ìŠˆ ë ˆì´ë¸” ìƒì„±
            true_labels = []
            for idx in range(len(embeddings)):
                issue_info = text_to_issue[idx]
                issue_id = issue_info.split('_')[0]
                issue_num = int(issue_id.split('_')[1]) if 'issue_' in issue_id else 0
                true_labels.append(issue_num)
            
            if len(set(true_labels)) > 1:
                issue_based_score = silhouette_score(normalized_embeddings, true_labels)
                print(f"    ğŸ“‹ ì‹¤ì œ ì´ìŠˆ ê¸°ë°˜ ì ìˆ˜: {issue_based_score:.4f}")
        except Exception as e:
            print(f"    âš ï¸ ì´ìŠˆ ê¸°ë°˜ í‰ê°€ ì‹¤íŒ¨: {e}")
        
        final_score = max(best_score, issue_based_score)
        
        print(f"    âœ… ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ì ìˆ˜: {final_score:.4f}")
        
        return {
            'clustering_score': float(final_score),
            'best_clustering_score': float(best_score),
            'issue_based_score': float(issue_based_score),
            'optimal_clusters': int(best_n_clusters),
            'actual_issues': int(num_issues),
            'debug_info': f"best_score={best_score:.4f}, issue_score={issue_based_score:.4f}"
        }
        
    except Exception as e:
        print(f"    âŒ í´ëŸ¬ìŠ¤í„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {
            'clustering_score': 0.0, 
            'best_clustering_score': 0.0,
            'issue_based_score': 0.0,
            'optimal_clusters': 0, 
            'actual_issues': 0,
            'debug_info': f"error: {str(e)}"
        }

def evaluate_testcase_performance(embeddings, text_to_issue, all_texts, embedding_list=None):
    """TestCase íŠ¹í™” ì„±ëŠ¥ í‰ê°€ (3ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ê¹Œì§€ë§Œ ì‚¬ìš©)"""
    
    print(f"\nğŸ¯ TestCase íŠ¹í™” ì„±ëŠ¥ í‰ê°€ (ì²˜ìŒ 3ê°œ ì´ìŠˆ ê¸°ì¤€)")
    
    # TestCase í‰ê°€ìš© ë°ì´í„° í•„í„°ë§ (ì²˜ìŒ 3ê°œ ì´ìŠˆë§Œ)
    testcase_indices = []
    testcase_texts = []
    testcase_text_to_issue = {}
    
    if embedding_list and len(embedding_list) >= 3:
        # ì²˜ìŒ 3ê°œ ì´ìŠˆì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        current_idx = 0
        for issue_idx in range(3):  # 0, 1, 2ë²ˆì§¸ ì´ìŠˆë§Œ
            if issue_idx < len(embedding_list):
                issue_text_list = embedding_list[issue_idx]
                if isinstance(issue_text_list, list):
                    for text_idx, text in enumerate(issue_text_list):
                        if text and isinstance(text, str) and text.strip():
                            testcase_indices.append(current_idx)
                            testcase_texts.append(text)
                            testcase_text_to_issue[len(testcase_texts)-1] = f"issue_{issue_idx}_text_{text_idx}"
                            current_idx += 1
                        else:
                            current_idx += 1
                else:
                    current_idx += 1
            else:
                break
    else:
        # embedding_listê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ì—ì„œ ì²˜ìŒ 3ê°œ ì´ìŠˆ ì¶”ì •
        issue_counts = {}
        for idx, issue_info in text_to_issue.items():
            issue_id = issue_info.split('_')[0]
            if issue_id not in issue_counts:
                issue_counts[issue_id] = 0
            issue_counts[issue_id] += 1
        
        # ì²˜ìŒ 3ê°œ ì´ìŠˆ ì„ íƒ
        first_three_issues = list(sorted(issue_counts.keys()))[:3]
        
        for idx, issue_info in text_to_issue.items():
            issue_id = issue_info.split('_')[0]
            if issue_id in first_three_issues:
                testcase_indices.append(idx)
                testcase_texts.append(all_texts[idx])
                testcase_text_to_issue[len(testcase_texts)-1] = issue_info
    
    if not testcase_indices:
        print(f"    âš ï¸ TestCase í‰ê°€ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {
            'testcase_functional_scores': {},
            'testcase_multilingual_score': 0.0,
            'testcase_technical_consistency': 0.0,
            'testcase_overall_score': 0.0,
            'functional_group_sizes': {},
            'testcase_data_used': 0
        }
    
    # TestCase í‰ê°€ìš© ì„ë² ë”© ì¶”ì¶œ
    testcase_embeddings = embeddings[testcase_indices]
    
    print(f"    ğŸ“Š TestCase í‰ê°€ ë°ì´í„°: {len(testcase_texts)}ê°œ (ì²˜ìŒ 3ê°œ ì´ìŠˆ)")
    
    # ê¸°ëŠ¥ë³„ ê·¸ë£¹ ì •ì˜ (TestCase ë°ì´í„°ë§Œìœ¼ë¡œ)
    functional_groups = {
        "ì‹œê°„_ë™ê¸°í™”": [],
        "ì¹´ë“œ_ì¸ì‹": [], 
        "ìŠ¤ì¼€ì¤„_ê´€ë¦¬": []
    }
    
    # í…ìŠ¤íŠ¸ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹ ë¶„ë¥˜
    for idx, text in enumerate(testcase_texts):
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in ["time synchronization", "ì‹œê°„ ë™ê¸°í™”", "dhcp", "time zone", "biostar server"]):
            functional_groups["ì‹œê°„_ë™ê¸°í™”"].append(idx)
        elif any(keyword in text_lower for keyword in ["hid prox", "fsk", "wiegand", "ì¹´ë“œ", "card format"]):
            functional_groups["ì¹´ë“œ_ì¸ì‹"].append(idx)
        elif any(keyword in text_lower for keyword in ["schedule", "ìŠ¤ì¼€ì¤„", "holiday", "time slot", "weekly", "daily"]):
            functional_groups["ìŠ¤ì¼€ì¤„_ê´€ë¦¬"].append(idx)
    
    print(f"    ğŸ“Š TestCase ê¸°ëŠ¥ë³„ ê·¸ë£¹ í¬ê¸°:")
    for group_name, indices in functional_groups.items():
        print(f"      - {group_name}: {len(indices)}ê°œ")
    
    # 1. ê¸°ëŠ¥ë³„ í´ëŸ¬ìŠ¤í„°ë§ ì ìˆ˜
    functional_scores = {}
    for group_name, indices in functional_groups.items():
        if len(indices) >= 2:
            group_embeddings = testcase_embeddings[indices]
            from sklearn.preprocessing import normalize
            normalized_embeddings = normalize(group_embeddings, norm='l2')
            
            # ê·¸ë£¹ ë‚´ í‰ê·  ìœ ì‚¬ë„
            similarities = cosine_similarity(normalized_embeddings)
            upper_triangle = similarities[np.triu_indices_from(similarities, k=1)]
            intra_similarity = np.mean(upper_triangle)
            functional_scores[group_name] = intra_similarity
            
            print(f"    ğŸ” {group_name} ê·¸ë£¹ ë‚´ ìœ ì‚¬ë„: {intra_similarity:.4f}")
    
    # 2. ë‹¤êµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ëŠ¥ (TestCase ë°ì´í„°ë§Œ)
    korean_keywords = ["ìŠ¤ì¼€ì¤„", "ì„¤ì •", "ì¸ì¦", "ì‹œê°„", "ì¹´ë“œ", "ì¥ì¹˜"]
    english_keywords = ["schedule", "setting", "authentication", "time", "card", "device"]
    
    multilingual_score = 0.0
    keyword_matches = 0
    
    for kr_word, en_word in zip(korean_keywords, english_keywords):
        kr_indices = [i for i, text in enumerate(testcase_texts) if kr_word in text]
        en_indices = [i for i, text in enumerate(testcase_texts) if en_word.lower() in text.lower()]
        
        if kr_indices and en_indices:
            # í•œêµ­ì–´-ì˜ì–´ í‚¤ì›Œë“œ ê°„ ìœ ì‚¬ë„
            kr_embeddings = testcase_embeddings[kr_indices]
            en_embeddings = testcase_embeddings[en_indices]
            
            cross_similarities = cosine_similarity(kr_embeddings, en_embeddings)
            avg_cross_similarity = np.mean(cross_similarities)
            multilingual_score += avg_cross_similarity
            keyword_matches += 1
    
    if keyword_matches > 0:
        multilingual_score /= keyword_matches
        print(f"    ğŸŒ ë‹¤êµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜: {multilingual_score:.4f}")
    
    # 3. ê¸°ìˆ  ìš©ì–´ ì¼ê´€ì„± ì ìˆ˜ (ìˆœìˆ˜ ê¸°ìˆ  ìš©ì–´ë§Œ ì‚¬ìš©)
    technical_terms = [
        "Time Slot",           # ìŠ¤ì¼€ì¤„ ê´€ë¦¬ ê¸°ìˆ  ìš©ì–´
        "Holiday",             # ìŠ¤ì¼€ì¤„ ê´€ë¦¬ ê¸°ìˆ  ìš©ì–´  
        "Device",              # í•˜ë“œì›¨ì–´ ê¸°ìˆ  ìš©ì–´
        "DHCP",                # ë„¤íŠ¸ì›Œí¬ ê¸°ìˆ  ìš©ì–´
        "Time Synchronization", # ì‹œê°„ ë™ê¸°í™” ê¸°ìˆ  ìš©ì–´
        "Wiegand",             # ì¹´ë“œ ì¸ì‹ ê¸°ìˆ  ìš©ì–´
        "Authentication",      # ì¸ì¦ ê¸°ìˆ  ìš©ì–´
        "Schedule"             # ìŠ¤ì¼€ì¤„ ê¸°ìˆ  ìš©ì–´
    ]
    
    technical_consistency = 0.0
    term_matches = 0
    
    for term in technical_terms:
        term_indices = [i for i, text in enumerate(testcase_texts) if term in text]
        if len(term_indices) >= 2:
            term_embeddings = testcase_embeddings[term_indices]
            similarities = cosine_similarity(term_embeddings)
            upper_triangle = similarities[np.triu_indices_from(similarities, k=1)]
            term_consistency = np.mean(upper_triangle)
            technical_consistency += term_consistency
            term_matches += 1
    
    if term_matches > 0:
        technical_consistency /= term_matches
        print(f"    ğŸ”§ ê¸°ìˆ  ìš©ì–´ ì¼ê´€ì„± ì ìˆ˜: {technical_consistency:.4f}")
    
    # 4. ì „ì²´ TestCase íŠ¹í™” ì ìˆ˜ ê³„ì‚°
    testcase_score = 0.0
    score_components = 0
    
    if functional_scores:
        avg_functional_score = np.mean(list(functional_scores.values()))
        testcase_score += avg_functional_score * 0.4  # 40% ê°€ì¤‘ì¹˜
        score_components += 1
        
    if multilingual_score > 0:
        testcase_score += multilingual_score * 0.3  # 30% ê°€ì¤‘ì¹˜
        score_components += 1
        
    if technical_consistency > 0:
        testcase_score += technical_consistency * 0.3  # 30% ê°€ì¤‘ì¹˜
        score_components += 1
    
    if score_components > 0:
        testcase_score = testcase_score  # ì´ë¯¸ ê°€ì¤‘í‰ê· ë¨
    
    print(f"    â­ TestCase íŠ¹í™” ì ìˆ˜ (3ê°œ ì´ìŠˆ ê¸°ì¤€): {testcase_score:.4f}")
    
    return {
        'testcase_functional_scores': functional_scores,
        'testcase_multilingual_score': multilingual_score,
        'testcase_technical_consistency': technical_consistency,
        'testcase_overall_score': testcase_score,
        'functional_group_sizes': {k: len(v) for k, v in functional_groups.items()},
        'testcase_data_used': len(testcase_texts)
    }

def evaluate_models_safely(embedding_list, model_names=None, max_texts_per_model=100):
    """
    ì•ˆì „í•˜ê²Œ ì—¬ëŸ¬ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        embedding_list: ì´ìŠˆë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        model_names: í‰ê°€í•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸
        max_texts_per_model: ëª¨ë¸ë‹¹ ìµœëŒ€ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ìˆ˜
    """
    
    if model_names is None:
        # í•œêµ­ì–´-ì˜ì–´ í˜¼í•© í™˜ê²½ ê¸°ë³¸ ëª¨ë¸ë“¤
        model_names = [
            'sentence-transformers/all-MiniLM-L6-v2',  # ì˜ì–´ ê¸°ì¤€ì„ 
            'BAAI/bge-m3',                              # ë‹¤êµ­ì–´ ìµœê³  ì„±ëŠ¥
            'jinaai/jina-embeddings-v3',                # í•œêµ­ì–´ íŠ¹í™”
            'Salesforce/SFR-Embedding-Mistral'         # SFR ëª¨ë¸
        ]
    
    print(f"ğŸš€ ë‹¤êµ­ì–´ ëª¨ë“œë¡œ {len(model_names)}ê°œ ëª¨ë¸ í‰ê°€ ì‹œì‘")
    print(f"ğŸ“Š ëª¨ë¸ë‹¹ ìµœëŒ€ {max_texts_per_model}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬")
    print(f"ğŸŒ í•œêµ­ì–´-ì˜ì–´ í˜¼í•© ë°ì´í„° ìµœì í™”")
    
    results = {}
    
    for i, model_name in enumerate(model_names, 1):
        print(f"\n[{i}/{len(model_names)}] {model_name}")
        print("-" * 60)
        
        # ë‹¤êµ­ì–´ ëª¨ë¸ì˜ ê²½ìš° ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if 'bge-m3' in model_name or 'e5-mistral' in model_name:
            batch_size = 1  # í° ë‹¤êµ­ì–´ ëª¨ë¸ì€ ë°°ì¹˜ í¬ê¸° ì¤„ì„
            max_texts = min(max_texts_per_model, 100)
        elif 'jina-embeddings-v3' in model_name or 'Salesforce/SFR-Embedding-Mistral' in model_name:
            batch_size = 2  # íš¨ìœ¨ì ì¸ ëª¨ë¸
            max_texts = max_texts_per_model
        else:
            batch_size = 4  # ì‘ì€ ëª¨ë¸
            max_texts = max_texts_per_model
        
        result = evaluate_single_model(
            embedding_list, 
            model_name, 
            max_texts=max_texts,
            batch_size=batch_size
        )
        
        results[model_name] = result
        
        # ê° ëª¨ë¸ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_memory()
        time.sleep(3)  # ë‹¤êµ­ì–´ ëª¨ë¸ì€ ì¡°ê¸ˆ ë” ëŒ€ê¸°
    
    # ê²°ê³¼ ì¶œë ¥
    print_results_table(results)
    
    return results

def print_results_table(results):
    """ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥ (Functional, Multi, Tech 3ê°œ ì§€í‘œ)"""
    
    print(f"\n{'='*130}")
    print("ğŸ† TestCase ì„ë² ë”© ëª¨ë¸ ì¢…í•© í‰ê°€ ê²°ê³¼")
    print(f"{'='*130}")
    
    # ì„±ê³µí•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§
    successful_results = {k: v for k, v in results.items() if v is not None}
    
    if not successful_results:
        print("âŒ ì„±ê³µì ìœ¼ë¡œ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê°œì„ ëœ í—¤ë” (TestCase ì œê±°, Functional ì¶”ê°€)
    print(f"{'Model':<35} {'Status':<8} {'Texts':<6} {'Speed':<10} {'Dim':<6} {'Functional':<10} {'Multi':<8} {'Tech':<8}")
    print("-" * 130)
    
    # ê° ëª¨ë¸ ê²°ê³¼
    for model_name, result in results.items():
        if result is None:
            row = f"{model_name:<35} {'FAILED':<8} {'-':<6} {'-':<10} {'-':<6} {'-':<10} {'-':<8} {'-':<8}"
        else:
            status = "SUCCESS"
            texts = str(result.get('total_texts', 0))
            speed = f"{result.get('texts_per_second', 0):.1f}t/s"
            dimension = str(result.get('embedding_dimension', 0))
            
            # Functional ì ìˆ˜ ê³„ì‚° (ê¸°ëŠ¥ë³„ ìœ ì‚¬ë„ì˜ í‰ê· )
            functional_scores = result.get('testcase_functional_scores', {})
            if functional_scores:
                functional = f"{np.mean(list(functional_scores.values())):.3f}"
            else:
                functional = "0.000"
            
            multilingual = f"{result.get('testcase_multilingual_score', 0):.3f}"
            technical = f"{result.get('testcase_technical_consistency', 0):.3f}"
            
            row = f"{model_name:<35} {status:<8} {texts:<6} {speed:<10} {dimension:<6} {functional:<10} {multilingual:<8} {technical:<8}"
        
        print(row)
    
    print(f"{'='*130}")
    print("ğŸ“Š ì§€í‘œ ì„¤ëª…:")
    print("  â€¢ Speed: ì„ë² ë”© ìƒì„± ì†ë„ (í…ìŠ¤íŠ¸/ì´ˆ)")
    print("  â€¢ Dim: ì„ë² ë”© ë²¡í„° ì°¨ì› ìˆ˜")
    print("  â€¢ Functional: ê¸°ëŠ¥ë³„ ê·¸ë£¹ ë‚´ ìœ ì‚¬ë„ (0~1)")
    print("  â€¢ Multi: ë‹¤êµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (0~1)")
    print("  â€¢ Tech: ê¸°ìˆ  ìš©ì–´ ì¼ê´€ì„± ì ìˆ˜ (0~1)")
    
    # ì„±ëŠ¥ vs íš¨ìœ¨ì„± ë¶„ì„
    print(f"\nâš¡ ì„±ëŠ¥ vs íš¨ìœ¨ì„± ë¶„ì„:")
    for model_name, result in successful_results.items():
        speed = result.get('texts_per_second', 0)
        functional_scores = result.get('testcase_functional_scores', {})
        functional = np.mean(list(functional_scores.values())) if functional_scores else 0
        dimension = result.get('embedding_dimension', 0)
        
        efficiency_ratio = speed / (dimension / 1000) if dimension > 0 else 0
        
        print(f"  ğŸ”¬ {model_name}:")
        print(f"    - ì„±ëŠ¥: Functional {functional:.3f} | Multi {result.get('testcase_multilingual_score', 0):.3f} | Tech {result.get('testcase_technical_consistency', 0):.3f}")
        print(f"    - íš¨ìœ¨ì„±: {speed:.1f}t/s | {dimension}ì°¨ì› | íš¨ìœ¨ë¹„ {efficiency_ratio:.2f}")
    
    # ìƒì„¸ ë¶„ì„
    print(f"\nğŸ“Š TestCase íŠ¹í™” ìƒì„¸ ë¶„ì„:")
    for model_name, result in successful_results.items():
        functional_scores = result.get('testcase_functional_scores', {})
        functional_avg = np.mean(list(functional_scores.values())) if functional_scores else 0
        
        print(f"\nğŸ”¬ {model_name}:")
        print(f"  â€¢ Functional ì ìˆ˜: {functional_avg:.4f}")
        
        # ê¸°ëŠ¥ë³„ ì ìˆ˜
        if functional_scores:
            print(f"  â€¢ ê¸°ëŠ¥ë³„ ìœ ì‚¬ë„:")
            for func, score in functional_scores.items():
                print(f"    - {func}: {score:.4f}")
        
        # ê·¸ë£¹ í¬ê¸°
        group_sizes = result.get('functional_group_sizes', {})
        if group_sizes:
            print(f"  â€¢ ê¸°ëŠ¥ë³„ í…ŒìŠ¤íŠ¸ ìˆ˜:")
            for func, size in group_sizes.items():
                print(f"    - {func}: {size}ê°œ")
        
        print(f"  â€¢ Multi ì ìˆ˜: {result.get('testcase_multilingual_score', 0):.4f}")
        print(f"  â€¢ Tech ì ìˆ˜: {result.get('testcase_technical_consistency', 0):.4f}")
        print(f"  â€¢ TestCase í‰ê°€ ë°ì´í„°: {result.get('testcase_data_used', 0)}ê°œ (ì²˜ìŒ 3ê°œ ì´ìŠˆ)")
        print(f"  â€¢ ì „ì²´ ì„ë² ë”© ë°ì´í„°: {result.get('total_texts', 0)}ê°œ")
        print(f"  â€¢ ì„ë² ë”© ì‹œê°„: {result.get('embedding_time', 0):.2f}ì´ˆ")
    
    # ì¢…í•© ì¶”ì²œ
    print(f"\nğŸ† ì¢…í•© ì¶”ì²œ (ì§€í‘œë³„):")
    
    # 1. Functional ê¸°ì¤€
    functional_rankings = []
    for model_name, result in successful_results.items():
        functional_scores = result.get('testcase_functional_scores', {})
        if functional_scores:
            functional_avg = np.mean(list(functional_scores.values()))
            functional_rankings.append((model_name, functional_avg))
    
    if functional_rankings:
        functional_rankings.sort(key=lambda x: x[1], reverse=True)
        print(f"  ğŸ¯ Functional (ê¸°ëŠ¥ë³„ ìœ ì‚¬ë„) ìˆœ:")
        for i, (model, score) in enumerate(functional_rankings[:3], 1):
            print(f"    {i}. {model}: {score:.4f}")
    
    # 2. Multi ê¸°ì¤€
    by_multilingual = sorted(successful_results.items(), 
                            key=lambda x: x[1].get('testcase_multilingual_score', 0), reverse=True)
    if by_multilingual:
        print(f"  ğŸŒ Multi (ë‹¤êµ­ì–´ ë§¤ì¹­) ìˆœ:")
        for i, (model, result) in enumerate(by_multilingual[:3], 1):
            score = result.get('testcase_multilingual_score', 0)
            print(f"    {i}. {model}: {score:.4f}")
    
    # 3. Tech ê¸°ì¤€
    by_technical = sorted(successful_results.items(), 
                         key=lambda x: x[1].get('testcase_technical_consistency', 0), reverse=True)
    if by_technical:
        print(f"  ğŸ”§ Tech (ê¸°ìˆ  ìš©ì–´ ì¼ê´€ì„±) ìˆœ:")
        for i, (model, result) in enumerate(by_technical[:3], 1):
            score = result.get('testcase_technical_consistency', 0)
            print(f"    {i}. {model}: {score:.4f}")
    
    print(f"\n{'='*130}")
    
    # í‰ê°€ ì§€í‘œ í•´ì„ ê°€ì´ë“œ
    print(f"\nğŸ“ í‰ê°€ ì§€í‘œ í•´ì„ ê°€ì´ë“œ:")
    print(f"  ğŸ¯ Functional + Multi + Tech = 3ê°€ì§€ ë…ë¦½ì  ì„±ëŠ¥ ì§€í‘œ")
    print(f"  ğŸ“Š ê° ì§€í‘œ 0~1 ë²”ìœ„, ë†’ì„ìˆ˜ë¡ í•´ë‹¹ ì¸¡ë©´ì—ì„œ ìš°ìˆ˜")
    print(f"  ğŸ” Functional: ê°™ì€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì„ë² ë”© ìœ ì‚¬ì„±")
    print(f"  ğŸŒ Multi: í•œêµ­ì–´-ì˜ì–´ í‚¤ì›Œë“œ ìŒì˜ ì˜ë¯¸ì  ë§¤ì¹­")
    print(f"  ğŸ”§ Tech: ë™ì¼ ê¸°ìˆ  ìš©ì–´ë“¤ì˜ ì„ë² ë”© ì¼ê´€ì„±")
    print(f"  ğŸ’¡ ìš©ë„ì— ë”°ë¼ ì¤‘ìš”í•œ ì§€í‘œë¥¼ ì„ íƒí•˜ì—¬ ëª¨ë¸ ê²°ì •")

def load_embedding_list_from_json(file_path):
    """
    JSON íŒŒì¼ì—ì„œ embedding_listë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        file_path: JSON íŒŒì¼ ê²½ë¡œ
    
    Returns:
        embedding_list: ì´ìŠˆë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    import json
    import os
    
    try:
        if not os.path.exists(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            print("ğŸ“ í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤:")
            for f in os.listdir('.'):
                if f.endswith('.json'):
                    print(f"  - {f}")
            return None
        
        print(f"ğŸ“‚ JSON íŒŒì¼ ë¡œë”© ì¤‘: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë‹¤ì–‘í•œ JSON êµ¬ì¡°ì— ëŒ€ì‘
        embedding_list = None
        
        # ì¼€ì´ìŠ¤ 1: ì§ì ‘ embedding_listê°€ ìˆëŠ” ê²½ìš°
        if 'embedding_list' in data:
            embedding_list = data['embedding_list']
            print(f"âœ… 'embedding_list' í‚¤ì—ì„œ ë°ì´í„° ë¡œë“œ")
        
        # ì¼€ì´ìŠ¤ 2: ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë˜ì–´ ìˆëŠ” ê²½ìš°
        elif isinstance(data, list):
            embedding_list = data
            print(f"âœ… ë£¨íŠ¸ ë ˆë²¨ì—ì„œ ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ")
        
        # ì¼€ì´ìŠ¤ 3: ì´ìŠˆë³„ë¡œ í‚¤ê°€ ìˆëŠ” ê²½ìš° (ì˜ˆ: {"issue_0": [...], "issue_1": [...]})
        elif isinstance(data, dict):
            # ì´ìŠˆ í‚¤ë“¤ì„ ì°¾ì•„ì„œ ì •ë ¬
            issue_keys = [k for k in data.keys() if k.startswith('issue_') or 'COMMONR-' in k]
            if issue_keys:
                # ìˆ«ì ìˆœìœ¼ë¡œ ì •ë ¬
                if 'COMMONR-' in issue_keys[0]:
                    issue_keys.sort(key=lambda x: int(x.split('-')[1]) if '-' in x and x.split('-')[1].isdigit() else 0)
                else:
                    issue_keys.sort(key=lambda x: int(x.split('_')[1]) if '_' in x and x.split('_')[1].isdigit() else 0)
                
                embedding_list = [data[key] for key in issue_keys]
                print(f"âœ… {len(issue_keys)}ê°œ ì´ìŠˆ í‚¤ì—ì„œ ë°ì´í„° ë¡œë“œ")
            else:
                # ëª¨ë“  ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                embedding_list = list(data.values())
                print(f"âœ… ë”•ì…”ë„ˆë¦¬ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜")
        
        if embedding_list is None:
            print(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” JSON êµ¬ì¡°ì…ë‹ˆë‹¤.")
            print(f"ğŸ“‹ ë°ì´í„° êµ¬ì¡°: {type(data)}")
            if isinstance(data, dict):
                print(f"ğŸ“‹ í‚¤ë“¤: {list(data.keys())[:5]}...")
            return None
        
        # ë°ì´í„° ê²€ì¦
        if not isinstance(embedding_list, list):
            print(f"âŒ embedding_listê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(embedding_list)}")
            return None
        
        # ê° ì´ìŠˆì˜ í…ìŠ¤íŠ¸ë“¤ í™•ì¸
        valid_issues = 0
        total_texts = 0
        
        for i, issue_texts in enumerate(embedding_list):
            if isinstance(issue_texts, list):
                text_count = sum(1 for text in issue_texts if isinstance(text, str) and text.strip())
                if text_count > 0:
                    valid_issues += 1
                    total_texts += text_count
        
        print(f"ğŸ“Š ë¡œë“œ ì™„ë£Œ:")
        print(f"  - ì´ ì´ìŠˆ ìˆ˜: {len(embedding_list)}")
        print(f"  - ìœ íš¨í•œ ì´ìŠˆ ìˆ˜: {valid_issues}")
        print(f"  - ì´ í…ìŠ¤íŠ¸ ìˆ˜: {total_texts}")
        
        if valid_issues == 0:
            print(f"âŒ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return embedding_list
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - JSONì—ì„œ ë°ì´í„° ë¡œë“œ"""
    
    # JSON íŒŒì¼ì—ì„œ embedding_list ë¡œë“œ
    print("ğŸ”§ ëª¨ë¸ í‰ê°€ í”„ë¡œê·¸ë¨ (TestCase íŠ¹í™”)")
    print("=" * 50)
    file_path = "evaluate_json/test_file.json"    
    embedding_list = load_embedding_list_from_json(file_path)
    
    if embedding_list is None:
        print("\nâš ï¸ JSON ë¡œë“œ ì‹¤íŒ¨, í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©")
        # í´ë°±: ì‘ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        embedding_list = [
            ["Login test step 1", "Login test step 2"],
            ["Dashboard test step 1", "Dashboard test step 2"],
            ["User management test step 1", "User management test step 2"]
        ]
        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(embedding_list)}ê°œ ì´ìŠˆ")
    
    # í•œêµ­ì–´-ì˜ì–´ í˜¼í•© í™˜ê²½ì— ìµœì í™”ëœ ë‹¤êµ­ì–´ ëª¨ë¸ë“¤
    target_models = [
        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',  # ë‹¤êµ­ì–´ ì‘ì€ ëª¨ë¸
        'sentence-transformers/all-MiniLM-L6-v2',  # ì˜ì–´ ê¸°ì¤€ì„ 
        'BAAI/bge-m3',
        'intfloat/multilingual-e5-large-instruct',                         
        'jinaai/jina-embeddings-v3',               
        'Salesforce/SFR-Embedding-Mistral'
    ]
    
    # ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ì „ëµ ê²°ì •
    total_texts = sum(len(issue_texts) for issue_texts in embedding_list if isinstance(issue_texts, list))
    
    if total_texts > 500:
        print(f"âš ï¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ê°ì§€ ({total_texts}ê°œ í…ìŠ¤íŠ¸)")
        print("ğŸ”„ í…ìŠ¤íŠ¸ ìˆ˜ë¥¼ ì œí•œí•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        max_texts = 200
    elif total_texts > 100:
        print(f"ğŸ“Š ì¤‘ê°„ í¬ê¸° ë°ì´í„° ({total_texts}ê°œ í…ìŠ¤íŠ¸)")
        max_texts = total_texts
    else:
        print(f"ğŸ“‹ ì†Œê·œëª¨ ë°ì´í„° ({total_texts}ê°œ í…ìŠ¤íŠ¸)")
        max_texts = total_texts
    
    # ëª©í‘œ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ (í•œ ë²ˆì— í•˜ë‚˜ì”©)
    print(f"\nğŸ¯ ëª¨ë¸ ê°œë³„ í…ŒìŠ¤íŠ¸ (ìµœëŒ€ {max_texts}ê°œ í…ìŠ¤íŠ¸)")
    target_results = {}
    
    for model in target_models:
        print(f"\nğŸ¯ {model} ê°œë³„ í…ŒìŠ¤íŠ¸")
        result = evaluate_models_safely(embedding_list, [model], max_texts_per_model=max_texts)
        target_results.update(result)
        
        # ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ ëª¨ë¸ë¡œ
        if result.get(model) is None:
            print(f"âš ï¸ {model} ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰")
            continue
    
    # ì „ì²´ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ ìµœì¢… ê²°ê³¼")
    print_results_table(target_results)

if __name__ == "__main__":
    main()