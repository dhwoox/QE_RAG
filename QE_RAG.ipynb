{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 1. Jira의 이슈 불러오기\n",
    "> 🔗 1. API 호출을 통해 Jira 데이터 가져오기\n",
    "- Jira의 이슈들을 API와 JQL을 통해 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (600325865.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    JIRA_USERNAME =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Jira에서 이슈들을 가져와서 JSONL 형태로 변환하는 스크립트 (임베딩용)\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Optional, Any\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# --- 설정 ---\n",
    "JIRA_URL = \"https://jira.suprema.co.kr\"\n",
    "JIRA_USERNAME =\n",
    "JIRA_PASSWORD = \n",
    "JIRA_AUTH = (JIRA_USERNAME, JIRA_PASSWORD)\n",
    "# 최종 결과가 저장될 폴더 이름\n",
    "OUTPUT_FOLDER = \"jira_issues_output\" # 저장 폴더 이름을 더 명확하게 변경\n",
    "\n",
    "# 검색할 JQL 쿼리\n",
    "JQL_QUERY = 'project = \"COMMONR\" AND issuetype = \"Test\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jira_issue_keys(jql):\n",
    "    \"\"\"JQL로 모든 이슈를 검색하여 이슈 키 리스트를 반환합니다.\"\"\"\n",
    "    all_issue_keys = []\n",
    "    start_at = 0\n",
    "    max_results = 100\n",
    "\n",
    "    print(f\"JQL로 이슈 검색을 시작합니다: {jql}\")\n",
    "\n",
    "    while True:\n",
    "        url = f\"{JIRA_URL}/rest/api/2/search\"\n",
    "        headers = {\"Accept\": \"application/json\"}\n",
    "        params = {\n",
    "            'jql': jql,\n",
    "            'fields': 'key',\n",
    "            'startAt': start_at,\n",
    "            'maxResults': max_results\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params, auth=JIRA_AUTH)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            issues_on_page = data.get('issues', [])\n",
    "            \n",
    "            if not issues_on_page:\n",
    "                break\n",
    "            \n",
    "            keys_on_page = [issue['key'] for issue in issues_on_page]\n",
    "            all_issue_keys.extend(keys_on_page)\n",
    "            \n",
    "            start_at += len(issues_on_page)\n",
    "            print(f\"  -> 현재까지 {len(all_issue_keys)} / {data['total']} 개 이슈 키를 가져왔습니다...\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Jira 이슈 검색 오류: {e}\")\n",
    "            return None\n",
    "            \n",
    "    return all_issue_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 2. JSON 파일 열 정제\n",
    "> 🔗 2. JSON 파일을 임베딩 모델에 임베딩하기 좋은 형식으로 저장\n",
    "- Jira의 이슈를 가져와서 임베딩 및 langchain에 가공하기 좋은 형식으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embedding_format(issue_key: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    하나의 이슈 키에 대한 모든 정보를 Jira API를 통해 가져와서\n",
    "    임베딩 및 ChromaDB 저장에 최적화된 형식으로 변환ㅁ합니다.\n",
    "    \n",
    "    Args:\n",
    "        issue_key (str): Jira 이슈 키\n",
    "        \n",
    "    Returns:\n",
    "        Optional[List[Dict[str, Any]]]: 임베딩용 객체들의 리스트\n",
    "        각 객체는 {'id', 'document', 'metadata'} 구조\n",
    "    \"\"\"\n",
    "    url = f\"{JIRA_URL}/rest/api/2/issue/{issue_key}\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, auth=JIRA_AUTH)\n",
    "        response.raise_for_status()\n",
    "        issue_data = response.json()\n",
    "        \n",
    "        if not issue_data:\n",
    "            return None\n",
    "        \n",
    "        # 이슈 데이터 추출\n",
    "        fields = issue_data.get('fields', {})\n",
    "        description = fields.get('description', '')\n",
    "        custom_field_10004 = fields.get('customfield_10004', {})\n",
    "        steps_list = custom_field_10004.get('steps', [])\n",
    "        \n",
    "        embedding_objects = []\n",
    "        \n",
    "        # 각 테스트 스텝별로 임베딩 객체 생성\n",
    "        if isinstance(steps_list, list):\n",
    "            for item in steps_list:\n",
    "                if isinstance(item, dict):\n",
    "                    index = item.get('index', '')\n",
    "                    step = item.get('step', '')\n",
    "                    data_item = item.get('data', '')\n",
    "                    expected_result = item.get('result', '')\n",
    "                    \n",
    "                    # page_content: test, step, data, expected result만\n",
    "                    content_parts = []\n",
    "                    if step:\n",
    "                        content_parts.append(f\"Test Step: {step}\")\n",
    "                    if data_item:\n",
    "                        content_parts.append(f\"Test Data: {data_item}\")\n",
    "                    if expected_result:\n",
    "                        content_parts.append(f\"Expected Result: {expected_result}\")\n",
    "                    \n",
    "                    if content_parts:\n",
    "                        step_object = {\n",
    "                            \"id\": f\"{issue_key}_step_{index}\",\n",
    "                            \"document\": \"\\n\".join(content_parts),  # ChromaDB 용어 사용\n",
    "                            \"metadata\": {\n",
    "                                \"issue_key\": issue_key,\n",
    "                                \"step_index\": str(index),  # ChromaDB는 문자열 선호\n",
    "                                \"source\": \"jira_test_step\"  # 데이터 소스 식별용\n",
    "                            }\n",
    "                        }\n",
    "                        embedding_objects.append(step_object)\n",
    "        \n",
    "        return embedding_objects\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"'{issue_key}'의 정보 조회 중 오류 발생: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_save_file(json_data,issue_key) : \n",
    "    #1. 결과를 저장할 폴더 생성\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    print(f\" {issue_key} 이슈의 정보를 json으로 저장을 시작합니다.\")\n",
    "    print(f\"저장 폴더: '{os.path.abspath(OUTPUT_FOLDER)}'\")\n",
    "\n",
    "    #2. json 파일로 저장장\n",
    "    try:\n",
    "        output_filepath = os.path.join(OUTPUT_FOLDER, f\"{issue_key}.json\")\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"'{output_filepath}' 파일 저장 중 오류 발생: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 3.JSON 파일 임베딩 모델을 통해 임베딩\n",
    "> 🔗 JSON 파일을 임베딩 모델을 통해서 임베딩\n",
    "- e5-multilingual 임베딩 모델을 통해서 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def embed_jira_documents_with_e5(jira_folder_path: str = \"jira_issues_output\",\n",
    "                                model_name: str = \"intfloat/multilingual-e5-large\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    jira_issues_output 폴더의 JSON 파일들을 읽어서 E5 모델로 임베딩합니다.\n",
    "    \n",
    "    Args:\n",
    "        jira_folder_path (str): JIRA JSON 파일들이 있는 폴더 경로\n",
    "        model_name (str): 사용할 E5 모델명\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: LangChain용으로 최적화된 임베딩 객체들\n",
    "            각 객체는 {'id', 'page_content', 'metadata', 'embedding'} 구조\n",
    "    \"\"\"\n",
    "    # 1. JSON 파일 읽기\n",
    "    if not os.path.exists(jira_folder_path):\n",
    "        raise FileNotFoundError(f\"폴더를 찾을 수 없습니다: {jira_folder_path}\")\n",
    "    \n",
    "    json_files = [f for f in os.listdir(jira_folder_path) if f.endswith('.json')]\n",
    "    if not json_files:\n",
    "        raise ValueError(f\"폴더에 JSON 파일이 없습니다: {jira_folder_path}\")\n",
    "    \n",
    "    all_documents = []\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(jira_folder_path, json_file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                all_documents.extend(data)\n",
    "            else:\n",
    "                all_documents.append(data)\n",
    "    \n",
    "    # 2. LangChain 형식으로 변환 (예외처리 포함)\n",
    "    embedding_objects = []\n",
    "    for doc in all_documents:\n",
    "        # 필수 키 체크 및 예외처리\n",
    "        if not doc or 'id' not in doc or 'document' not in doc or 'metadata' not in doc:\n",
    "            continue  # 빈 객체나 필수 키가 없으면 건너뛰기\n",
    "            \n",
    "        if not doc['document'].strip():  # 빈 문서도 건너뛰기\n",
    "            continue\n",
    "            \n",
    "        embedding_obj = {\n",
    "            'id': doc['id'],\n",
    "            'page_content': doc['document'],\n",
    "            'metadata': doc['metadata']\n",
    "        }\n",
    "        embedding_objects.append(embedding_obj)\n",
    "    \n",
    "    # 3. E5 모델 로드 및 임베딩\n",
    "    print(f\"E5 모델 로딩 중: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    print(f\"사용 디바이스: {device}\")\n",
    "    \n",
    "    # page_content 추출하여 임베딩\n",
    "    documents = [obj['page_content'] for obj in embedding_objects]\n",
    "    \n",
    "    print(f\"총 {len(documents)}개 문서 임베딩 시작...\")\n",
    "    \n",
    "    prefixed_documents = [f\"passage: {doc}\" for doc in documents]\n",
    "    \n",
    "    embeddings = model.encode(\n",
    "        prefixed_documents,\n",
    "        batch_size=8,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    print(f\"임베딩 완료! 벡터 차원: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # 4. 임베딩 결과 추가\n",
    "    result_objects = []\n",
    "    for i, obj in enumerate(embedding_objects):\n",
    "        embedded_obj = obj.copy()\n",
    "        embedded_obj['embedding'] = embeddings[i].tolist()\n",
    "        result_objects.append(embedded_obj)\n",
    "    \n",
    "    return result_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 4. 벡터 DB를 통해서 저장\n",
    "> 🔗 해당 임베딩한 것을 벡터 DB를 통해서 저장\n",
    "- Chroma DB를 사용해서 임베딩 값 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5 모델 로딩 중: intfloat/multilingual-e5-large\n",
      "사용 디바이스: cpu\n",
      "총 2632개 문서 임베딩 시작...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b83510d6b4a472a905b0200a3d848b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 완료! 벡터 차원: 1024\n",
      "✅ 2632개 문서가 ChromaDB 컬렉션 'jira_test_cases'에 저장되었습니다.\n",
      "📁 저장 위치: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def save_embeddings_to_chroma(embedded_objects: List[Dict[str, Any]], \n",
    "    collection_name: str = \"jira_test_cases\",\n",
    "    persist_directory: str = \"./chroma_db\") -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    임베딩된 객체들을 ChromaDB에 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        embedded_objects (List[Dict[str, Any]]): 임베딩이 포함된 객체들\n",
    "            각 객체는 {'id', 'page_content', 'metadata', 'embedding'} 구조\n",
    "        collection_name (str): ChromaDB 컬렉션 이름\n",
    "        persist_directory (str): ChromaDB 데이터 저장 경로\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: 생성된 ChromaDB 컬렉션\n",
    "    \"\"\"\n",
    "    # ChromaDB 클라이언트 생성 (영구 저장)\n",
    "    client = chromadb.PersistentClient(path=persist_directory)\n",
    "    \n",
    "    # 기존 컬렉션이 있으면 삭제 후 새로 생성\n",
    "    try:\n",
    "        client.delete_collection(name=collection_name)\n",
    "        print(f\"기존 컬렉션 '{collection_name}' 삭제됨\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 새 컬렉션 생성\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # 코사인 유사도 사용\n",
    "    )\n",
    "    \n",
    "    # 데이터 분리\n",
    "    ids = [obj['id'] for obj in embedded_objects]\n",
    "    documents = [obj['page_content'] for obj in embedded_objects]\n",
    "    metadatas = [obj['metadata'] for obj in embedded_objects]\n",
    "    embeddings = [obj['embedding'] for obj in embedded_objects]\n",
    "    \n",
    "    # ChromaDB에 배치 추가\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ {len(embedded_objects)}개 문서가 ChromaDB 컬렉션 '{collection_name}'에 저장되었습니다.\")\n",
    "    print(f\"📁 저장 위치: {persist_directory}\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "embedded_data = embed_jira_documents_with_e5()\n",
    "collection = save_embeddings_to_chroma(embedded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 5-1. langchain을 이용한 구축 with LM Studio\n",
    "> 🔗 langchain을 이용해서 저장한 임베딩 값을 Langchain, LM Studio와 연계하여 최대의 값 도출\n",
    "- langchain을 이용하여 chroma DB 결과값 QA RAG로 구현\n",
    "- 해당 RAG LM Studio LLM과 연동하여 기대값에 대한 결과값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 기존 ChromaDB 컬렉션 'jira_test_cases' 연결 완료\n",
      "✅ LM Studio 연결 성공! 사용 가능한 모델: 2개\n",
      "🚀 LangChain 테스트케이스 시스템 종합 테스트 시작\n",
      "============================================================\n",
      "\n",
      "🔍 테스트케이스 찾기 테스트\n",
      "----------------------------------------\n",
      "🔍 테스트케이스 검색 중: Master Admin과 관련된 테스트케이스 중에서 master admin 설정 갯수에 대한 테스트케이스를 가져와줘\n",
      "\n",
      "📋 검색 결과:\n",
      "==================================================\n",
      "🔍 검색 쿼리: Master Admin과 관련된 테스트케이스 중에서 master admin 설정 갯수에 대한 테스트케이스를 가져와줘\n",
      "📊 찾은 테스트케이스 수: 10개\n",
      "\n",
      "💡 분석 결과:\n",
      "Error: LM Studio 통신 오류 - HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=120)\n",
      "\n",
      "📚 찾은 테스트케이스들:\n",
      "\n",
      "1. COMMONR-380_step_1\n",
      "   Test Step: 1. Device> 관리자 설정\n",
      "Test Data: 1. Master Password 설정\n",
      "2. 전체 관리자 설정\n",
      "3. 장치 설정 관리자 설정\n",
      "4. 사용자 관리자 설정\n",
      "Expected Result: 1. 사용자가 선택한 관리자로 설정되어야 한다.\n",
      ">...\n",
      "\n",
      "2. COMMONR-218_step_5\n",
      "   Test Step: [Master Admin 지원 신규HW]\n",
      "1. Master Admin 설정\n",
      "2. Secure Tamper: Enable 설정\n",
      "3. Secure Tamper 발생\n",
      "Test Data: UI 지원모델\n",
      "\n",
      "※ Device> Device Info> Mac을 길...\n",
      "\n",
      "3. COMMONR-247_step_1\n",
      "   Test Step: 1. 장치 두대이상 선택> Batch Edit 클릭\n",
      "2. 다수의 관리자(All/User/Config) 최대로 설정 후 적용\n",
      "3. 장치상세정보창 진입\n",
      "4. 관리자 확인\n",
      "5. All/User/Config 관리자로 메뉴진입시도\n",
      "Test Data: > Ma...\n",
      "\n",
      "4. COMMONR-379_step_18\n",
      "   Test Step: [Door 생성]\n",
      "1. Door > Add Door > Door 구성\n",
      "2. Manual Lock 동작\n",
      "3. Admin Menu 로그인 시도(Step 2에서 등록한 Lock Override) > Master장치에 인증 시도\n",
      "Test Data: 1. C...\n",
      "\n",
      "5. COMMONR-247_step_2\n",
      "   Test Step: 1. 장치 두대이상 선택> Batch Edit 클릭\n",
      "2. All/User/Config 관리자를 1명씩 설정 후 적용\n",
      "3. 장치상세정보창 진입\n",
      "4. 관리자 확인\n",
      "5. All/User/Config 관리자로 메뉴진입시도\n",
      "Test Data: > Max 관리...\n",
      "\n",
      "==============================\n",
      "\n",
      "\n",
      "🚀 테스트케이스 생성 테스트\n",
      "----------------------------------------\n",
      "🚀 테스트케이스 생성 중: 장치에 전체 관리자 설정을 강제할 수 있는 Master Admin 기능이 있는데 이 기능은 다음과 같이 동작을 해. 다만 이 기능이 동작을 하기 위해서는 조건이 있어. 버전이 V1.4.0 이상으로 생산된 제품이어야해. 조건에 부합되는 장치의 전원이 인가되면 화면에 Master Admin 설정화면이 표시가 돼. 하지만 버전이 V1.4.0 이하로 생산된 제품의 경우에는 장치 전원이 인가되면 메인화면이 표시가 돼. 버전은 BS3의 이전 버전들을 참고해서 테스트 케이스로 작성해줘.\n",
      "\n",
      "🚀 생성 결과:\n",
      "==================================================\n",
      "📝 요구사항: 장치에 전체 관리자 설정을 강제할 수 있는 Master Admin 기능이 있는데 이 기능은 다음과 같이 동작을 해. 다만 이 기능이 동작을 하기 위해서는 조건이 있어. 버전이 V1.4.0 이상으로 생산된 제품이어야해. 조건에 부합되는 장치의 전원이 인가되면 화면에 Master Admin 설정화면이 표시가 돼. 하지만 버전이 V1.4.0 이하로 생산된 제품의 경우에는 장치 전원이 인가되면 메인화면이 표시가 돼. 버전은 BS3의 이전 버전들을 참고해서 테스트 케이스로 작성해줘.\n",
      "📊 참고한 테스트케이스 수: 8개\n",
      "\n",
      "🎯 생성된 테스트케이스:\n",
      "Error: LM Studio 통신 오류 - HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=120)\n",
      "\n",
      "==============================\n",
      "\n",
      "✅ LangChain 결과가 langchain_testcase_results_20250810_233713.json에 저장되었습니다.\n",
      "\n",
      "🎉 LangChain 종합 테스트 완료!\n",
      "총 2개의 결과가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "# FutureWarning 무시\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "class LMStudioLLM(LLM):\n",
    "    \"\"\"LM Studio와 연동하는 LangChain 호환 LLM 클래스\"\"\"\n",
    "    \n",
    "    base_url: str = \"http://127.0.0.1:1234/v1\"\n",
    "    model_name: str = \"qwen/qwen3-8b\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 2048\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_url: str = \"http://127.0.0.1:1234/v1\",\n",
    "                 model_name: str = \"qwen/qwen3-8b\",\n",
    "                 temperature: float = 0.1,\n",
    "                 max_tokens: int = 2048,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self._test_connection()\n",
    "    \n",
    "    def _test_connection(self):\n",
    "        \"\"\"LM Studio 연결 테스트\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/models\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json()\n",
    "                print(f\"✅ LM Studio 연결 성공! 사용 가능한 모델: {len(models.get('data', []))}개\")\n",
    "            else:\n",
    "                print(f\"⚠️ LM Studio 연결 상태 확인 필요: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ LM Studio 연결 실패: {e}\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"lm_studio\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            if stop:\n",
    "                payload[\"stop\"] = stop\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=30000\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            else:\n",
    "                return f\"Error: LM Studio 응답 오류 (status: {response.status_code})\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error: LM Studio 통신 오류 - {str(e)}\"\n",
    "\n",
    "\n",
    "class LangChainTestCaseSystem:\n",
    "    def __init__(self, \n",
    "                 persist_directory: str = \"./chroma_db\",\n",
    "                 collection_name: str = \"jira_test_cases\",\n",
    "                 embedding_model_name: str = \"intfloat/multilingual-e5-large\",\n",
    "                 lm_studio_url: str = \"http://127.0.0.1:1234/v1\",\n",
    "                 lm_studio_model: str = \"qwen/qwen3-8b\"):\n",
    "        \n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # 임베딩 모델 설정 (CPU 사용)\n",
    "        model_kwargs = {'device': 'cpu', 'trust_remote_code': True}\n",
    "        encode_kwargs = {'normalize_embeddings': True, 'batch_size': 4}\n",
    "        \n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        \n",
    "        # ChromaDB 연결\n",
    "        self.vectorstore = self._connect_to_chroma()\n",
    "        \n",
    "        # LM Studio LLM 초기화\n",
    "        self.llm = LMStudioLLM(\n",
    "            base_url=lm_studio_url,\n",
    "            model_name=lm_studio_model,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        \n",
    "        # 테스트케이스 찾기용 체인\n",
    "        self.search_chain = self._setup_search_chain()\n",
    "        \n",
    "        # 테스트케이스 생성용 체인\n",
    "        self.generation_chain = self._setup_generation_chain()\n",
    "        \n",
    "        # 결과 저장용\n",
    "        self.test_results = []\n",
    "    \n",
    "    def _connect_to_chroma(self) -> Chroma:\n",
    "        \"\"\"기존 ChromaDB 컬렉션에 연결\"\"\"\n",
    "        try:\n",
    "            vectorstore = Chroma(\n",
    "                collection_name=self.collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                persist_directory=self.persist_directory\n",
    "            )\n",
    "            print(f\"✅ 기존 ChromaDB 컬렉션 '{self.collection_name}' 연결 완료\")\n",
    "            return vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ChromaDB 연결 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_search_chain(self) -> RetrievalQA:\n",
    "        \"\"\"테스트케이스 검색용 체인 설정\"\"\"\n",
    "        search_prompt_template = PromptTemplate(\n",
    "            template=\"\"\"당신은 테스트케이스 검색 전문가입니다. 사용자의 요청에 맞는 테스트케이스를 찾아서 정리해주세요.\n",
    "\n",
    "검색된 테스트케이스들:\n",
    "{context}\n",
    "\n",
    "사용자 요청: {question}\n",
    "\n",
    "답변 가이드라인:\n",
    "1. 요청과 관련된 테스트케이스들을 명확하게 정리해주세요\n",
    "2. 각 테스트케이스의 이슈 키, 테스트 데이터, 테스트 스텝, 예상 결과를 포함해주세요\n",
    "3. 이슈별로 그룹핑하여 체계적으로 보여주세요\n",
    "4. 찾은 테스트케이스가 충분하지 않다면 추가 검색 키워드를 제안해주세요\n",
    "\n",
    "답변:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={\"score_threshold\": 0.6, \"k\": 10}\n",
    "            ),\n",
    "            chain_type_kwargs={\"prompt\": search_prompt_template},\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def _setup_generation_chain(self) -> RetrievalQA:\n",
    "        \"\"\"테스트케이스 생성용 체인 설정\"\"\"\n",
    "        generation_prompt_template = PromptTemplate(\n",
    "            template=\"\"\"당신은 테스트케이스 작성 전문가입니다. 기존 테스트케이스들을 참고하여 새로운 테스트케이스를 작성해주세요.\n",
    "\n",
    "참고할 기존 테스트케이스들:\n",
    "{context}\n",
    "\n",
    "사용자 요청: {question}\n",
    "\n",
    "새로운 테스트케이스 작성 가이드라인:\n",
    "1. 사용자 요청에 맞는 구체적인 테스트케이스를 작성해주세요\n",
    "2. 테스트 목적, 전제 조건, 테스트 스텝, 예상 결과를 명확히 구분해주세요\n",
    "3. 기존 테스트케이스의 패턴과 형식을 참고하되, 새로운 시나리오를 제안해주세요\n",
    "4. 테스트 데이터는 구체적이고 현실적으로 작성해주세요\n",
    "5. Edge Case나 예외 상황도 고려해주세요\n",
    "6. 가능하다면 여러 개의 테스트케이스 시나리오를 제안해주세요\n",
    "\n",
    "테스트케이스 형식:\n",
    "**테스트 케이스: [테스트 제목]**\n",
    "- **테스트 목적**: [목적 설명]\n",
    "- **전제 조건**: [사전 조건]\n",
    "- **테스트 스텝**:\n",
    "  1. [스텝 1]\n",
    "  2. [스텝 2]\n",
    "  ...\n",
    "- **테스트 데이터**: [필요한 데이터]\n",
    "- **예상 결과**: [기대하는 결과]\n",
    "\n",
    "답변:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": 8}\n",
    "            ),\n",
    "            chain_type_kwargs={\"prompt\": generation_prompt_template},\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def find_test_cases(self, query: str, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"테스트케이스 찾기 기능\"\"\"\n",
    "        try:\n",
    "            print(f\"🔍 테스트케이스 검색 중: {query}\")\n",
    "            \n",
    "            formatted_query = f\"query: {query}\"\n",
    "            response = self.search_chain({\"query\": formatted_query})\n",
    "            \n",
    "            # 소스 문서 정리\n",
    "            source_docs = []\n",
    "            for doc in response.get(\"source_documents\", []):\n",
    "                source_docs.append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"issue_key\": doc.metadata.get(\"issue_key\", \"\"),\n",
    "                    \"step_index\": doc.metadata.get(\"step_index\", \"\")\n",
    "                })\n",
    "            \n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"answer\": response[\"result\"],\n",
    "                \"found_test_cases\": source_docs,\n",
    "                \"total_found\": len(source_docs)\n",
    "            }\n",
    "            \n",
    "            # 결과 저장\n",
    "            self._save_result(\"find_test_cases\", query, result)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"테스트케이스 검색 중 오류: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            error_result = {\"query\": query, \"error\": error_msg}\n",
    "            self._save_result(\"find_test_cases\", query, error_result)\n",
    "            return error_result\n",
    "    \n",
    "    def generate_test_case(self, requirement: str) -> Dict[str, Any]:\n",
    "        \"\"\"테스트케이스 생성 기능\"\"\"\n",
    "        try:\n",
    "            print(f\"🚀 테스트케이스 생성 중: {requirement}\")\n",
    "            \n",
    "            formatted_requirement = f\"query: {requirement}\"\n",
    "            response = self.generation_chain({\"query\": formatted_requirement})\n",
    "            \n",
    "            # 참고한 소스 문서들\n",
    "            reference_docs = []\n",
    "            for doc in response.get(\"source_documents\", []):\n",
    "                reference_docs.append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"issue_key\": doc.metadata.get(\"issue_key\", \"\"),\n",
    "                    \"step_index\": doc.metadata.get(\"step_index\", \"\")\n",
    "                })\n",
    "            \n",
    "            result = {\n",
    "                \"requirement\": requirement,\n",
    "                \"generated_test_case\": response[\"result\"],\n",
    "                \"reference_test_cases\": reference_docs,\n",
    "                \"reference_count\": len(reference_docs)\n",
    "            }\n",
    "            \n",
    "            # 결과 저장\n",
    "            self._save_result(\"generate_test_case\", requirement, result)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"테스트케이스 생성 중 오류: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            error_result = {\"requirement\": requirement, \"error\": error_msg}\n",
    "            self._save_result(\"generate_test_case\", requirement, error_result)\n",
    "            return error_result\n",
    "    \n",
    "    def _save_result(self, test_type: str, query: str, result: Any):\n",
    "        \"\"\"결과 저장\"\"\"\n",
    "        self.test_results.append({\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"framework\": \"langchain\",\n",
    "            \"test_type\": test_type,\n",
    "            \"query\": query,\n",
    "            \"result\": result\n",
    "        })\n",
    "    \n",
    "    def save_results_to_file(self, filename: str = None):\n",
    "        \"\"\"결과를 파일에 저장\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"langchain_testcase_results_{timestamp}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.test_results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"✅ LangChain 결과가 {filename}에 저장되었습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "    \n",
    "    def print_find_result(self, result: Dict[str, Any]):\n",
    "        \"\"\"테스트케이스 찾기 결과 출력\"\"\"\n",
    "        print(f\"\\n📋 검색 결과:\")\n",
    "        print(\"=\" * 50)\n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ 오류: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🔍 검색 쿼리: {result['query']}\")\n",
    "        print(f\"📊 찾은 테스트케이스 수: {result['total_found']}개\")\n",
    "        print(f\"\\n💡 분석 결과:\")\n",
    "        print(result['answer'])\n",
    "        \n",
    "        if result['found_test_cases']:\n",
    "            print(f\"\\n📚 찾은 테스트케이스들:\")\n",
    "            for i, doc in enumerate(result['found_test_cases'][:5], 1):\n",
    "                issue_key = doc['issue_key']\n",
    "                step_idx = doc['step_index']\n",
    "                print(f\"\\n{i}. {issue_key}_step_{step_idx}\")\n",
    "                print(f\"   {doc['content'][:150]}...\")\n",
    "    \n",
    "    def print_generation_result(self, result: Dict[str, Any]):\n",
    "        \"\"\"테스트케이스 생성 결과 출력\"\"\"\n",
    "        print(f\"\\n🚀 생성 결과:\")\n",
    "        print(\"=\" * 50)\n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ 오류: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📝 요구사항: {result['requirement']}\")\n",
    "        print(f\"📊 참고한 테스트케이스 수: {result['reference_count']}개\")\n",
    "        print(f\"\\n🎯 생성된 테스트케이스:\")\n",
    "        print(result['generated_test_case'])\n",
    "    \n",
    "    def run_comprehensive_test(self):\n",
    "        \"\"\"종합 테스트 실행\"\"\"\n",
    "        print(\"🚀 LangChain 테스트케이스 시스템 종합 테스트 시작\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. 테스트케이스 찾기 테스트들\n",
    "        find_queries = [\n",
    "            \"Master Admin과 관련된 테스트케이스 중에서 master admin 설정 갯수에 대한 테스트케이스를 가져와줘\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n🔍 테스트케이스 찾기 테스트\")\n",
    "        print(\"-\" * 40)\n",
    "        for query in find_queries:\n",
    "            result = self.find_test_cases(query)\n",
    "            self.print_find_result(result)\n",
    "            print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        # 2. 테스트케이스 생성 테스트들\n",
    "        generation_requirements = [\n",
    "            \"장치에 전체 관리자 설정을 강제할 수 있는 Master Admin 기능이 있는데 이 기능은 다음과 같이 동작을 해. 다만 이 기능이 동작을 하기 위해서는 조건이 있어. 버전이 V1.4.0 이상으로 생산된 제품이어야해. 조건에 부합되는 장치의 전원이 인가되면 화면에 Master Admin 설정화면이 표시가 돼. 하지만 버전이 V1.4.0 이하로 생산된 제품의 경우에는 장치 전원이 인가되면 메인화면이 표시가 돼. 버전은 BS3의 이전 버전들을 참고해서 테스트 케이스로 작성해줘.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n🚀 테스트케이스 생성 테스트\")\n",
    "        print(\"-\" * 40)\n",
    "        for requirement in generation_requirements:\n",
    "            result = self.generate_test_case(requirement)\n",
    "            self.print_generation_result(result)\n",
    "            print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        self.save_results_to_file()\n",
    "        \n",
    "        print(f\"\\n🎉 LangChain 종합 테스트 완료!\")\n",
    "        print(f\"총 {len(self.test_results)}개의 결과가 저장되었습니다.\")\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # LangChain 테스트케이스 시스템 초기화\n",
    "    langchain_system = LangChainTestCaseSystem(\n",
    "        lm_studio_url=\"http://127.0.0.1:1234/v1\",\n",
    "        lm_studio_model=\"qwen/qwen3-8b\"\n",
    "    )\n",
    "    \n",
    "    # 종합 테스트 실행\n",
    "    langchain_system.run_comprehensive_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 5-2. llamaindex을 이용한 구축 with LM Studio\n",
    "> 🔗 llamaindex을 이용해서 저장한 임베딩 값을 Langchain, LM Studio와 연계하여 최대의 값 도출\n",
    "- llamaindex을 이용하여 chroma DB 결과값 QA RAG로 구현\n",
    "- 해당 RAG LM Studio LLM과 연동하여 기대값에 대한 결과값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LM Studio 연결 성공! 사용 가능한 모델: 2개\n",
      "✅ LM Studio LLM 설정 완료: http://127.0.0.1:1234/v1\n",
      "✅ 기존 ChromaDB 컬렉션 'jira_test_cases' 연결 완료\n",
      "✅ LlamaIndex 벡터 인덱스 생성 완료\n",
      "✅ 검색용 쿼리 엔진 설정 완료\n",
      "✅ 생성용 쿼리 엔진 설정 완료\n",
      "🚀 LlamaIndex 테스트케이스 시스템 종합 테스트 시작\n",
      "============================================================\n",
      "\n",
      "🔍 고급 테스트케이스 검색 테스트\n",
      "----------------------------------------\n",
      "🔍 LlamaIndex 고급 검색 중: Master Admin과 관련된 테스트케이스 중에서 master admin 설정 갯수에 대한 테스트케이스를 가져와줘\n",
      "\n",
      "🔍 LlamaIndex 검색 결과:\n",
      "==================================================\n",
      "📝 검색 쿼리: Master Admin과 관련된 테스트케이스 중에서 master admin 설정 갯수에 대한 테스트케이스를 가져와줘\n",
      "📊 찾은 테스트케이스: 20개\n",
      "🏷️ 관련 이슈: 11개\n",
      "\n",
      "🤖 LLM 분석 결과:\n",
      "Error: LM Studio 통신 오류 - HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=120)\n",
      "\n",
      "📚 이슈별 테스트케이스:\n",
      "\n",
      "🔗 COMMONR-380:\n",
      "  1. (점수: 0.892) Test Step: 1. Device> 관리자 설정\n",
      "Test Data: 1. Master Password 설정\n",
      "2. 전체 관리자 설정\n",
      "3. 장치 설정 관리자 설정\n",
      "4. 사용자 관리자 설정\n",
      "Expected Result...\n",
      "  2. (점수: 0.861) Test Step: 1. 관리자 설정/미설정\n",
      "2. 메뉴 진입 상태 > 인증 \n",
      "3. 메뉴 진입 시도와 동시에 인증 시도\n",
      "Expected Result: 1. 설정된 권한으로 출입문제어가 되어야 한다.\n",
      "> 메뉴 진입 후 ...\n",
      "\n",
      "🔗 COMMONR-218:\n",
      "  1. (점수: 0.872) Test Step: [Master Admin 지원 신규HW]\n",
      "1. Master Admin 설정\n",
      "2. Secure Tamper: Enable 설정\n",
      "3. Secure Tamper 발생\n",
      "Test Data: UI 지원모델\n",
      "...\n",
      "  2. (점수: 0.860) Test Step: [Master Admin 지원 신규HW]\n",
      "1. Secure Tamper: Enable 설정\n",
      "2. Secure Tamper 발생\n",
      "Test Data: UI 미지원모델 - ex. XP2\n",
      "Expected...\n",
      "\n",
      "🔗 COMMONR-247:\n",
      "  1. (점수: 0.872) Test Step: 1. 장치 두대이상 선택> Batch Edit 클릭\n",
      "2. 다수의 관리자(All/User/Config) 최대로 설정 후 적용\n",
      "3. 장치상세정보창 진입\n",
      "4. 관리자 확인\n",
      "5. All/User/Conf...\n",
      "  2. (점수: 0.869) Test Step: 1. 장치 두대이상 선택> Batch Edit 클릭\n",
      "2. All/User/Config 관리자를 1명씩 설정 후 적용\n",
      "3. 장치상세정보창 진입\n",
      "4. 관리자 확인\n",
      "5. All/User/Config 관...\n",
      "\n",
      "==============================\n",
      "\n",
      "\n",
      "🚀 고급 테스트케이스 생성 테스트\n",
      "----------------------------------------\n",
      "🚀 LlamaIndex 고급 생성 중: 장치에 전체 관리자 설정을 강제할 수 있는 Master Admin 기능이 있는데 이 기능은 다음과 같이 동작을 해. 다만 이 기능이 동작을 하기 위해서는 조건이 있어. 버전이 V1.4.0 이상으로 생산된 제품이어야해. 조건에 부합되는 장치의 전원이 인가되면 화면에 Master Admin 설정화면이 표시가 돼. 하지만 버전이 V1.4.0 이하로 생산된 제품의 경우에는 장치 전원이 인가되면 메인화면이 표시가 돼. 버전은 BS3의 이전 버전들을 참고해서 테스트 케이스로 작성해줘.\n",
      "\n",
      "🚀 LlamaIndex 생성 결과:\n",
      "==================================================\n",
      "📋 요구사항: 장치에 전체 관리자 설정을 강제할 수 있는 Master Admin 기능이 있는데 이 기능은 다음과 같이 동작을 해. 다만 이 기능이 동작을 하기 위해서는 조건이 있어. 버전이 V1.4.0 이상으로 생산된 제품이어야해. 조건에 부합되는 장치의 전원이 인가되면 화면에 Master Admin 설정화면이 표시가 돼. 하지만 버전이 V1.4.0 이하로 생산된 제품의 경우에는 장치 전원이 인가되면 메인화면이 표시가 돼. 버전은 BS3의 이전 버전들을 참고해서 테스트 케이스로 작성해줘.\n",
      "📊 참조 자료: 15개\n",
      "⭐ 생성 품질 점수: 20/100\n",
      "🔍 참조 분석: 분석된 15개 참조 자료에서 1개 이슈 타입 발견\n",
      "\n",
      "🎯 생성된 테스트케이스:\n",
      "----------------------------------------\n",
      "Error: LM Studio 통신 오류 - HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=120)\n",
      "\n",
      "==============================\n",
      "\n",
      "✅ LlamaIndex 결과가 llamaindex_testcase_results_20250810_234123.json에 저장되었습니다.\n",
      "\n",
      "🎉 LlamaIndex 종합 테스트 완료!\n",
      "총 2개의 결과가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, LLMRerank\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from typing import List, Dict, Any, Optional, Generator, Sequence\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "# FutureWarning 무시\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "class LMStudioLLM(CustomLLM):\n",
    "    \"\"\"LM Studio와 연동하는 LlamaIndex용 커스텀 LLM 클래스\"\"\"\n",
    "    \n",
    "    model_name: str = \"qwen/qwen3-8b\"\n",
    "    base_url: str = \"http://127.0.0.1:1234/v1\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 2048\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_url: str = \"http://127.0.0.1:1234/v1\",\n",
    "                 model_name: str = \"qwen/qwen3-8b\",\n",
    "                 temperature: float = 0.1,\n",
    "                 max_tokens: int = 2048,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self._test_connection()\n",
    "    \n",
    "    def _test_connection(self):\n",
    "        \"\"\"LM Studio 연결 테스트\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/models\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json()\n",
    "                print(f\"✅ LM Studio 연결 성공! 사용 가능한 모델: {len(models.get('data', []))}개\")\n",
    "            else:\n",
    "                print(f\"⚠️ LM Studio 연결 상태 확인 필요: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ LM Studio 연결 실패: {e}\")\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return LLMMetadata(\n",
    "            context_window=8192,\n",
    "            num_output=self.max_tokens,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=30000\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return CompletionResponse(text=text)\n",
    "            else:\n",
    "                error_text = f\"Error: LM Studio 응답 오류 (status: {response.status_code})\"\n",
    "                return CompletionResponse(text=error_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_text = f\"Error: LM Studio 통신 오류 - {str(e)}\"\n",
    "            return CompletionResponse(text=error_text)\n",
    "    \n",
    "    def chat(self, messages: Sequence[ChatMessage], **kwargs) -> CompletionResponse:\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            formatted_messages.append({\n",
    "                \"role\": msg.role.value if hasattr(msg.role, 'value') else str(msg.role),\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        \n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": formatted_messages,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=30000\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return CompletionResponse(text=text)\n",
    "            else:\n",
    "                error_text = f\"Error: LM Studio 응답 오류 (status: {response.status_code})\"\n",
    "                return CompletionResponse(text=error_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_text = f\"Error: LM Studio 통신 오류 - {str(e)}\"\n",
    "            return CompletionResponse(text=error_text)\n",
    "    \n",
    "    def stream_complete(self, prompt: str, **kwargs) -> Generator[CompletionResponse, None, None]:\n",
    "        response = self.complete(prompt, **kwargs)\n",
    "        yield response\n",
    "    \n",
    "    def stream_chat(self, messages: Sequence[ChatMessage], **kwargs) -> Generator[CompletionResponse, None, None]:\n",
    "        response = self.chat(messages, **kwargs)\n",
    "        yield response\n",
    "\n",
    "\n",
    "class LlamaIndexTestCaseSystem:\n",
    "    def __init__(self, \n",
    "                 persist_directory: str = \"./chroma_db\",\n",
    "                 collection_name: str = \"jira_test_cases\",\n",
    "                 embedding_model_name: str = \"intfloat/multilingual-e5-large\",\n",
    "                 lm_studio_url: str = \"http://127.0.0.1:1234/v1\",\n",
    "                 lm_studio_model: str = \"qwen/qwen3-8b\"):\n",
    "        \n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.lm_studio_url = lm_studio_url\n",
    "        self.lm_studio_model = lm_studio_model\n",
    "        \n",
    "        # Global Settings 설정\n",
    "        self._setup_global_settings(embedding_model_name)\n",
    "        \n",
    "        # ChromaDB 벡터 스토어 연결\n",
    "        self.vector_store = self._connect_to_chroma()\n",
    "        \n",
    "        # 인덱스 생성\n",
    "        self.index = self._create_index()\n",
    "        \n",
    "        # 테스트케이스 찾기용 쿼리 엔진\n",
    "        self.search_engine = self._setup_search_engine()\n",
    "        \n",
    "        # 테스트케이스 생성용 쿼리 엔진\n",
    "        self.generation_engine = self._setup_generation_engine()\n",
    "        \n",
    "        # 결과 저장용\n",
    "        self.test_results = []\n",
    "    \n",
    "    def _setup_global_settings(self, embedding_model_name: str):\n",
    "        \"\"\"LlamaIndex Global Settings 설정\"\"\"\n",
    "        # 임베딩 모델 설정 (CPU 사용)\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=embedding_model_name,\n",
    "            device=\"cpu\",\n",
    "            trust_remote_code=True,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # LM Studio LLM 설정\n",
    "        Settings.llm = LMStudioLLM(\n",
    "            base_url=self.lm_studio_url,\n",
    "            model_name=self.lm_studio_model,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ LM Studio LLM 설정 완료: {self.lm_studio_url}\")\n",
    "    \n",
    "    def _connect_to_chroma(self) -> ChromaVectorStore:\n",
    "        \"\"\"기존 ChromaDB 컬렉션에 연결\"\"\"\n",
    "        try:\n",
    "            chroma_client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            chroma_collection = chroma_client.get_collection(name=self.collection_name)\n",
    "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "            \n",
    "            print(f\"✅ 기존 ChromaDB 컬렉션 '{self.collection_name}' 연결 완료\")\n",
    "            return vector_store\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ChromaDB 연결 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_index(self) -> VectorStoreIndex:\n",
    "        \"\"\"기존 벡터 스토어로부터 인덱스 생성\"\"\"\n",
    "        try:\n",
    "            storage_context = StorageContext.from_defaults(vector_store=self.vector_store)\n",
    "            index = VectorStoreIndex.from_vector_store(\n",
    "                vector_store=self.vector_store,\n",
    "                storage_context=storage_context\n",
    "            )\n",
    "            \n",
    "            print(\"✅ LlamaIndex 벡터 인덱스 생성 완료\")\n",
    "            return index\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 인덱스 생성 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_search_engine(self) -> RetrieverQueryEngine:\n",
    "        \"\"\"테스트케이스 검색용 쿼리 엔진 설정\"\"\"\n",
    "        # 검색에 특화된 리트리버 (더 많은 결과, 낮은 임계값)\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=12\n",
    "        )\n",
    "        \n",
    "        # 검색용 포스트프로세서 (관련성 높은 결과만 필터링)\n",
    "        postprocessors = [\n",
    "            SimilarityPostprocessor(similarity_cutoff=0.6)\n",
    "        ]\n",
    "        \n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=postprocessors\n",
    "        )\n",
    "        \n",
    "        # 검색용 커스텀 프롬프트\n",
    "        search_prompt = PromptTemplate(\n",
    "            \"\"\"당신은 테스트케이스 검색 및 분석 전문가입니다. 사용자의 요청에 맞는 테스트케이스를 찾아서 체계적으로 정리해주세요.\n",
    "\n",
    "검색된 테스트케이스들:\n",
    "{context_str}\n",
    "\n",
    "사용자 검색 요청: {query_str}\n",
    "\n",
    "검색 결과 분석 가이드라인:\n",
    "1. 요청과 관련된 테스트케이스들을 명확하게 정리해주세요\n",
    "2. 각 테스트케이스의 이슈 키, 테스트 데이터, 테스트 스텝, 예상 결과를 포함해주세요\n",
    "3. 이슈별로 그룹핑하여 체계적으로 보여주세요\n",
    "4. 찾은 테스트케이스가 충분하지 않다면 추가 검색 키워드를 제안해주세요\n",
    "\n",
    "답변:\"\"\"\n",
    "        )\n",
    "        \n",
    "        query_engine.update_prompts({\"response_synthesizer:text_qa_template\": search_prompt})\n",
    "        \n",
    "        print(\"✅ 검색용 쿼리 엔진 설정 완료\")\n",
    "        return query_engine\n",
    "    \n",
    "    def _setup_generation_engine(self) -> RetrieverQueryEngine:\n",
    "        \"\"\"테스트케이스 생성용 쿼리 엔진 설정\"\"\"\n",
    "        # 생성에 특화된 리트리버 (적은 수의 고품질 예시)\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=8\n",
    "        )\n",
    "        \n",
    "        # 생성용 포스트프로세서 (높은 품질의 참고 자료만 선별)\n",
    "        postprocessors = [\n",
    "            SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "        ]\n",
    "        \n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=postprocessors\n",
    "        )\n",
    "        \n",
    "        # 생성용 커스텀 프롬프트\n",
    "        generation_prompt = PromptTemplate(\n",
    "            \"\"\"당신은 테스트케이스 작성 전문가입니다. 기존 테스트케이스들의 패턴과 구조를 분석하여 새로운 고품질 테스트케이스를 작성해주세요.\n",
    "\n",
    "참고할 기존 테스트케이스들:\n",
    "{context_str}\n",
    "\n",
    "테스트케이스 작성 요청: {query_str}\n",
    "\n",
    "새로운 테스트케이스 작성 가이드라인:\n",
    "1. 사용자 요청에 맞는 구체적인 테스트케이스를 작성해주세요\n",
    "2. 테스트 목적, 전제 조건, 테스트 스텝, 예상 결과를 명확히 구분해주세요\n",
    "3. 기존 테스트케이스의 패턴과 형식을 참고하되, 새로운 시나리오를 제안해주세요\n",
    "4. 테스트 데이터는 구체적이고 현실적으로 작성해주세요\n",
    "5. Edge Case나 예외 상황도 고려해주세요\n",
    "6. 가능하다면 여러 개의 테스트케이스 시나리오를 제안해주세요\n",
    "\n",
    "테스트케이스 형식:\n",
    "**테스트 케이스: [테스트 제목]**\n",
    "- **테스트 목적**: [목적 설명]\n",
    "- **전제 조건**: [사전 조건]\n",
    "- **테스트 스텝**:\n",
    "  1. [스텝 1]\n",
    "  2. [스텝 2]\n",
    "  ...\n",
    "- **테스트 데이터**: [필요한 데이터]\n",
    "- **예상 결과**: [기대하는 결과]\n",
    "\n",
    "답변:\"\"\"\n",
    "        )\n",
    "        \n",
    "        query_engine.update_prompts({\"response_synthesizer:text_qa_template\": generation_prompt})\n",
    "        \n",
    "        print(\"✅ 생성용 쿼리 엔진 설정 완료\")\n",
    "        return query_engine\n",
    "    \n",
    "    def find_test_cases(self, query: str, use_advanced_search: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"고급 테스트케이스 검색 기능\"\"\"\n",
    "        try:\n",
    "            print(f\"🔍 LlamaIndex 고급 검색 중: {query}\")\n",
    "            \n",
    "            formatted_query = f\"query: {query}\"\n",
    "            \n",
    "            if use_advanced_search:\n",
    "                # 고급 검색: 다단계 검색 및 재랭킹\n",
    "                \n",
    "                # 1단계: 넓은 범위 검색\n",
    "                broad_retriever = VectorIndexRetriever(\n",
    "                    index=self.index,\n",
    "                    similarity_top_k=20\n",
    "                )\n",
    "                broad_nodes = broad_retriever.retrieve(formatted_query)\n",
    "                \n",
    "                # 2단계: 관련성 기반 필터링\n",
    "                similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.5)\n",
    "                filtered_nodes = similarity_processor.postprocess_nodes(broad_nodes)\n",
    "                \n",
    "                # 검색 결과를 직접 처리\n",
    "                search_result = self._process_search_nodes(filtered_nodes, query)\n",
    "                \n",
    "                # LLM을 통한 분석\n",
    "                analysis_response = self.search_engine.query(formatted_query)\n",
    "                search_result[\"llm_analysis\"] = str(analysis_response)\n",
    "                \n",
    "            else:\n",
    "                # 기본 검색\n",
    "                response = self.search_engine.query(formatted_query)\n",
    "                search_result = {\n",
    "                    \"query\": query,\n",
    "                    \"llm_analysis\": str(response),\n",
    "                    \"found_nodes\": []\n",
    "                }\n",
    "                \n",
    "                if hasattr(response, 'source_nodes'):\n",
    "                    search_result[\"found_nodes\"] = self._process_search_nodes(response.source_nodes, query)[\"found_nodes\"]\n",
    "            \n",
    "            # 결과 저장\n",
    "            self._save_result(\"find_test_cases\", query, search_result)\n",
    "            \n",
    "            return search_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"테스트케이스 검색 중 오류: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            error_result = {\"query\": query, \"error\": error_msg}\n",
    "            self._save_result(\"find_test_cases\", query, error_result)\n",
    "            return error_result\n",
    "    \n",
    "    def generate_test_case(self, requirement: str, use_advanced_generation: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"고급 테스트케이스 생성 기능\"\"\"\n",
    "        try:\n",
    "            print(f\"🚀 LlamaIndex 고급 생성 중: {requirement}\")\n",
    "            \n",
    "            formatted_requirement = f\"query: {requirement}\"\n",
    "            \n",
    "            if use_advanced_generation:\n",
    "                # 고급 생성: 참조 자료 품질 최적화\n",
    "                \n",
    "                # 1단계: 관련 테스트케이스 검색\n",
    "                reference_retriever = VectorIndexRetriever(\n",
    "                    index=self.index,\n",
    "                    similarity_top_k=15\n",
    "                )\n",
    "                reference_nodes = reference_retriever.retrieve(formatted_requirement)\n",
    "                \n",
    "                # 2단계: 고품질 참조 자료만 선별\n",
    "                quality_processor = SimilarityPostprocessor(similarity_cutoff=0.8)\n",
    "                quality_nodes = quality_processor.postprocess_nodes(reference_nodes)\n",
    "                \n",
    "                # 참조 자료 분석\n",
    "                reference_analysis = self._analyze_reference_nodes(quality_nodes)\n",
    "                \n",
    "                # LLM을 통한 테스트케이스 생성\n",
    "                generation_response = self.generation_engine.query(formatted_requirement)\n",
    "                \n",
    "                generation_result = {\n",
    "                    \"requirement\": requirement,\n",
    "                    \"generated_test_case\": str(generation_response),\n",
    "                    \"reference_analysis\": reference_analysis,\n",
    "                    \"reference_count\": len(quality_nodes),\n",
    "                    \"generation_quality_score\": self._calculate_generation_quality(str(generation_response))\n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                # 기본 생성\n",
    "                response = self.generation_engine.query(formatted_requirement)\n",
    "                generation_result = {\n",
    "                    \"requirement\": requirement,\n",
    "                    \"generated_test_case\": str(response),\n",
    "                    \"reference_count\": 0\n",
    "                }\n",
    "            \n",
    "            # 결과 저장\n",
    "            self._save_result(\"generate_test_case\", requirement, generation_result)\n",
    "            \n",
    "            return generation_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"테스트케이스 생성 중 오류: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            error_result = {\"requirement\": requirement, \"error\": error_msg}\n",
    "            self._save_result(\"generate_test_case\", requirement, error_result)\n",
    "            return error_result\n",
    "    \n",
    "    def _process_search_nodes(self, nodes, query):\n",
    "        \"\"\"검색 노드들을 처리하여 구조화된 결과 생성\"\"\"\n",
    "        found_nodes = []\n",
    "        issue_groups = {}\n",
    "        \n",
    "        for node in nodes:\n",
    "            node_info = {\n",
    "                \"content\": node.text,\n",
    "                \"metadata\": node.metadata,\n",
    "                \"score\": float(node.score) if node.score else 0.0,\n",
    "                \"issue_key\": node.metadata.get(\"issue_key\", \"\"),\n",
    "                \"step_index\": node.metadata.get(\"step_index\", \"\")\n",
    "            }\n",
    "            found_nodes.append(node_info)\n",
    "            \n",
    "            # 이슈별 그룹핑\n",
    "            issue_key = node.metadata.get(\"issue_key\", \"UNKNOWN\")\n",
    "            if issue_key not in issue_groups:\n",
    "                issue_groups[issue_key] = []\n",
    "            issue_groups[issue_key].append(node_info)\n",
    "        \n",
    "        # 이슈별 정렬\n",
    "        for issue_key in issue_groups:\n",
    "            issue_groups[issue_key].sort(\n",
    "                key=lambda x: int(x[\"step_index\"]) if x[\"step_index\"].isdigit() else 0\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"found_nodes\": found_nodes,\n",
    "            \"issue_groups\": issue_groups,\n",
    "            \"total_found\": len(found_nodes),\n",
    "            \"unique_issues\": len(issue_groups)\n",
    "        }\n",
    "    \n",
    "    def _analyze_reference_nodes(self, nodes):\n",
    "        \"\"\"참조 노드들을 분석하여 패턴 추출\"\"\"\n",
    "        if not nodes:\n",
    "            return {\"pattern_analysis\": \"참조 자료가 충분하지 않습니다.\"}\n",
    "        \n",
    "        patterns = {\n",
    "            \"common_steps\": [],\n",
    "            \"test_data_patterns\": [],\n",
    "            \"result_patterns\": [],\n",
    "            \"issue_types\": set()\n",
    "        }\n",
    "        \n",
    "        for node in nodes:\n",
    "            # 이슈 타입 수집\n",
    "            issue_key = node.metadata.get(\"issue_key\", \"\")\n",
    "            if issue_key:\n",
    "                patterns[\"issue_types\"].add(issue_key.split(\"-\")[0] if \"-\" in issue_key else issue_key)\n",
    "            \n",
    "            # 텍스트 패턴 분석 (간단한 키워드 기반)\n",
    "            content = node.text.lower()\n",
    "            if \"step\" in content:\n",
    "                patterns[\"common_steps\"].append(content[:100])\n",
    "            if \"data\" in content:\n",
    "                patterns[\"test_data_patterns\"].append(content[:100])\n",
    "            if \"result\" in content or \"expected\" in content:\n",
    "                patterns[\"result_patterns\"].append(content[:100])\n",
    "        \n",
    "        patterns[\"issue_types\"] = list(patterns[\"issue_types\"])\n",
    "        \n",
    "        return {\n",
    "            \"pattern_analysis\": f\"분석된 {len(nodes)}개 참조 자료에서 {len(patterns['issue_types'])}개 이슈 타입 발견\",\n",
    "            \"patterns\": patterns,\n",
    "            \"reference_quality\": len(nodes)\n",
    "        }\n",
    "    \n",
    "    def _calculate_generation_quality(self, generated_text):\n",
    "        \"\"\"생성된 테스트케이스의 품질 점수 계산 (간단한 휴리스틱)\"\"\"\n",
    "        quality_score = 0\n",
    "        \n",
    "        # 기본 구조 확인\n",
    "        if \"테스트\" in generated_text:\n",
    "            quality_score += 10\n",
    "        if \"스텝\" in generated_text or \"단계\" in generated_text:\n",
    "            quality_score += 15\n",
    "        if \"예상\" in generated_text or \"결과\" in generated_text:\n",
    "            quality_score += 15\n",
    "        if \"데이터\" in generated_text:\n",
    "            quality_score += 10\n",
    "        if \"조건\" in generated_text:\n",
    "            quality_score += 10\n",
    "        \n",
    "        # 길이 기반 점수 (너무 짧거나 길지 않은 적절한 길이)\n",
    "        text_length = len(generated_text)\n",
    "        if 200 <= text_length <= 2000:\n",
    "            quality_score += 20\n",
    "        elif text_length > 100:\n",
    "            quality_score += 10\n",
    "        \n",
    "        # 구체성 점수 (숫자나 구체적인 용어 포함)\n",
    "        import re\n",
    "        if re.search(r'\\d+', generated_text):\n",
    "            quality_score += 10\n",
    "        if len(re.findall(r'[가-힣]{2,}', generated_text)) > 10:\n",
    "            quality_score += 10\n",
    "        \n",
    "        return min(quality_score, 100)  # 최대 100점\n",
    "    \n",
    "    def _save_result(self, test_type: str, query: str, result: Any):\n",
    "        \"\"\"결과 저장\"\"\"\n",
    "        self.test_results.append({\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"framework\": \"llamaindex\",\n",
    "            \"test_type\": test_type,\n",
    "            \"query\": query,\n",
    "            \"result\": result\n",
    "        })\n",
    "    \n",
    "    def save_results_to_file(self, filename: str = None):\n",
    "        \"\"\"결과를 파일에 저장\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"llamaindex_testcase_results_{timestamp}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.test_results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"✅ LlamaIndex 결과가 {filename}에 저장되었습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "    \n",
    "    def print_find_result(self, result: Dict[str, Any]):\n",
    "        \"\"\"테스트케이스 찾기 결과 출력\"\"\"\n",
    "        print(f\"\\n🔍 LlamaIndex 검색 결과:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ 오류: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📝 검색 쿼리: {result['query']}\")\n",
    "        \n",
    "        if \"found_nodes\" in result:\n",
    "            print(f\"📊 찾은 테스트케이스: {result.get('total_found', 0)}개\")\n",
    "            print(f\"🏷️ 관련 이슈: {result.get('unique_issues', 0)}개\")\n",
    "        \n",
    "        if \"llm_analysis\" in result:\n",
    "            print(f\"\\n🤖 LLM 분석 결과:\")\n",
    "            print(result['llm_analysis'])\n",
    "        \n",
    "        if \"issue_groups\" in result:\n",
    "            print(f\"\\n📚 이슈별 테스트케이스:\")\n",
    "            for issue_key, nodes in list(result['issue_groups'].items())[:3]:  # 상위 3개만 출력\n",
    "                print(f\"\\n🔗 {issue_key}:\")\n",
    "                for i, node in enumerate(nodes[:2], 1):  # 각 이슈당 2개까지만\n",
    "                    score = node.get('score', 0)\n",
    "                    print(f\"  {i}. (점수: {score:.3f}) {node['content'][:120]}...\")\n",
    "    \n",
    "    def print_generation_result(self, result: Dict[str, Any]):\n",
    "        \"\"\"테스트케이스 생성 결과 출력\"\"\"\n",
    "        print(f\"\\n🚀 LlamaIndex 생성 결과:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ 오류: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📋 요구사항: {result['requirement']}\")\n",
    "        print(f\"📊 참조 자료: {result.get('reference_count', 0)}개\")\n",
    "        \n",
    "        if \"generation_quality_score\" in result:\n",
    "            quality = result['generation_quality_score']\n",
    "            print(f\"⭐ 생성 품질 점수: {quality}/100\")\n",
    "        \n",
    "        if \"reference_analysis\" in result:\n",
    "            ref_analysis = result['reference_analysis']\n",
    "            print(f\"🔍 참조 분석: {ref_analysis.get('pattern_analysis', 'N/A')}\")\n",
    "        \n",
    "        print(f\"\\n🎯 생성된 테스트케이스:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result['generated_test_case'])\n",
    "    \n",
    "    def run_comprehensive_test(self):\n",
    "        \"\"\"LlamaIndex 종합 테스트 실행\"\"\"\n",
    "        print(\"🚀 LlamaIndex 테스트케이스 시스템 종합 테스트 시작\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. 고급 테스트케이스 찾기 테스트들\n",
    "        find_queries = [\n",
    "            \"Master Admin과 관련된 테스트케이스 중에서 master admin 설정 갯수에 대한 테스트케이스를 가져와줘\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n🔍 고급 테스트케이스 검색 테스트\")\n",
    "        print(\"-\" * 40)\n",
    "        for query in find_queries:\n",
    "            result = self.find_test_cases(query, use_advanced_search=True)\n",
    "            self.print_find_result(result)\n",
    "            print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        # 2. 고급 테스트케이스 생성 테스트들\n",
    "        generation_requirements = [\n",
    "            \"장치에 전체 관리자 설정을 강제할 수 있는 Master Admin 기능이 있는데 이 기능은 다음과 같이 동작을 해. 다만 이 기능이 동작을 하기 위해서는 조건이 있어. 버전이 V1.4.0 이상으로 생산된 제품이어야해. 조건에 부합되는 장치의 전원이 인가되면 화면에 Master Admin 설정화면이 표시가 돼. 하지만 버전이 V1.4.0 이하로 생산된 제품의 경우에는 장치 전원이 인가되면 메인화면이 표시가 돼. 버전은 BS3의 이전 버전들을 참고해서 테스트 케이스로 작성해줘.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n🚀 고급 테스트케이스 생성 테스트\")\n",
    "        print(\"-\" * 40)\n",
    "        for requirement in generation_requirements:\n",
    "            result = self.generate_test_case(requirement, use_advanced_generation=True)\n",
    "            self.print_generation_result(result)\n",
    "            print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        self.save_results_to_file()\n",
    "        \n",
    "        print(f\"\\n🎉 LlamaIndex 종합 테스트 완료!\")\n",
    "        print(f\"총 {len(self.test_results)}개의 결과가 저장되었습니다.\")\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # LlamaIndex 테스트케이스 시스템 초기화\n",
    "    llamaindex_system = LlamaIndexTestCaseSystem(\n",
    "        lm_studio_url=\"http://127.0.0.1:1234/v1\",\n",
    "        lm_studio_model=\"qwen/qwen3-8b\"\n",
    "    )\n",
    "    \n",
    "    # 종합 테스트 실행\n",
    "    llamaindex_system.run_comprehensive_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMAINDEX WITH LANGCHAIN\n",
    "# Test\n",
    "# Test\n",
    "# Test\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext, Settings\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters, FilterOperator\n",
    "\n",
    "# LangChain 관련 임포트\n",
    "from langchain_community.llms import OpenAI # LM Studio를 OpenAI API 호환으로 사용할 경우\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate as LCPromptTemplate # LangChain의 PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from llama_index.core.langchain_helpers import LlamaIndexRetriever # LlamaIndex 리트리버를 LangChain에 통합\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Generator, Sequence\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "# LangChain DeprecationWarning 무시 (해결되기를 기다리며 임시 조치)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module='langchain_core._api.deprecation')\n",
    "\n",
    "\n",
    "class LMStudioLLM(CustomLLM):\n",
    "    \"\"\"LM Studio와 연동하는 LlamaIndex용 커스텀 LLM 클래스\"\"\"\n",
    "    \n",
    "    model_name: str = \"qwen/qwen3-8b\"\n",
    "    base_url: str = \"http://127.0.0.1:1234/v1\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 2048\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_url: str = \"http://127.0.0.1:1234/v1\",\n",
    "                 model_name: str = \"qwen/qwen3-8b\",\n",
    "                 temperature: float = 0.1,\n",
    "                 max_tokens: int = 2048,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self._test_connection()\n",
    "    \n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/models\", timeout=100000)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json()\n",
    "                print(f\"✅ LM Studio 연결 성공! 사용 가능한 모델: {len(models.get('data', []))}개\")\n",
    "            else:\n",
    "                print(f\"⚠️ LM Studio 연결 상태 확인 필요: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ LM Studio 연결 실패: {e}\")\n",
    "            print(\"LM Studio가 실행 중인지 확인해주세요.\")\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return LLMMetadata(\n",
    "            context_window=8192,\n",
    "            num_output=self.max_tokens,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=100000\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return CompletionResponse(text=text)\n",
    "            else:\n",
    "                error_text = f\"Error: LM Studio 응답 오류 (status: {response.status_code})\"\n",
    "                return CompletionResponse(text=error_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_text = f\"Error: LM Studio 통신 오류 - {str(e)}\"\n",
    "            return CompletionResponse(text=error_text)\n",
    "    \n",
    "    def chat(self, messages: Sequence[ChatMessage], **kwargs) -> CompletionResponse:\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            formatted_messages.append({\n",
    "                \"role\": msg.role.value if hasattr(msg.role, 'value') else str(msg.role),\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        \n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"messages\": formatted_messages,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return CompletionResponse(text=text)\n",
    "            else:\n",
    "                error_text = f\"Error: LM Studio 응답 오류 (status: {response.status_code})\"\n",
    "                return CompletionResponse(text=error_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_text = f\"Error: LM Studio 통신 오류 - {str(e)}\"\n",
    "            return CompletionResponse(text=error_text)\n",
    "    \n",
    "    def stream_complete(self, prompt: str, **kwargs) -> Generator[CompletionResponse, None, None]:\n",
    "        response = self.complete(prompt, **kwargs)\n",
    "        yield response\n",
    "    \n",
    "    def stream_chat(self, messages: Sequence[ChatMessage], **kwargs) -> Generator[CompletionResponse, None, None]:\n",
    "        response = self.chat(messages, **kwargs)\n",
    "        yield response\n",
    "\n",
    "\n",
    "class TestCaseRAGLlamaIndexLMStudio:\n",
    "    def __init__(self, \n",
    "                 persist_directory: str = \"./chroma_db\",\n",
    "                 collection_name: str = \"jira_test_cases\",\n",
    "                 embedding_model_name: str = \"intfloat/multilingual-e5-large\",\n",
    "                 lm_studio_url: str = \"http://localhost:1234/v1\",\n",
    "                 lm_studio_model: str = \"local-model\"):\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.lm_studio_url = lm_studio_url\n",
    "        self.lm_studio_model = lm_studio_model\n",
    "        \n",
    "        self._setup_global_settings(embedding_model_name)\n",
    "        self.vector_store = self._connect_to_chroma()\n",
    "        self.index = self._create_index()\n",
    "        self.query_engine = self._setup_query_engine()\n",
    "        self._setup_custom_prompts()\n",
    "    \n",
    "    def _setup_global_settings(self, embedding_model_name: str):\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=embedding_model_name,\n",
    "            device=\"cpu\",\n",
    "            trust_remote_code=True,\n",
    "            normalize=True,\n",
    "            query_instruction=\"query:\",  \n",
    "            text_instruction=\"passage:\", \n",
    "        )\n",
    "        \n",
    "        Settings.llm = LMStudioLLM(\n",
    "            base_url=self.lm_studio_url,\n",
    "            model_name=self.lm_studio_model,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        print(f\"✅ LM Studio LLM 설정 완료: {self.lm_studio_url}\")\n",
    "    \n",
    "    def _connect_to_chroma(self) -> ChromaVectorStore:\n",
    "        try:\n",
    "            chroma_client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            chroma_collection = chroma_client.get_or_create_collection(name=self.collection_name)\n",
    "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "            print(f\"✅ 기존 ChromaDB 컬렉션 '{self.collection_name}' 연결 완료\")\n",
    "            return vector_store\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ChromaDB 연결 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_index(self) -> VectorStoreIndex:\n",
    "        try:\n",
    "            storage_context = StorageContext.from_defaults(vector_store=self.vector_store)\n",
    "            index = VectorStoreIndex.from_vector_store(\n",
    "                vector_store=self.vector_store,\n",
    "                storage_context=storage_context\n",
    "            )\n",
    "            print(\"✅ LlamaIndex 벡터 인덱스 생성 완료\")\n",
    "            return index\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 인덱스 생성 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_query_engine(self) -> RetrieverQueryEngine:\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=8,\n",
    "            vector_store_query_mode=\"default\"\n",
    "        )\n",
    "        postprocessors = [\n",
    "            SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "        ]\n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=postprocessors\n",
    "        )\n",
    "        print(\"✅ LM Studio 쿼리 엔진 설정 완료\")\n",
    "        return query_engine\n",
    "    \n",
    "    def _setup_custom_prompts(self):\n",
    "        qa_prompt_template = PromptTemplate(\n",
    "            \"\"\"당신은 테스트케이스 전문가입니다. 주어진 테스트 스텝들을 바탕으로 사용자의 질문에 정확하고 유용한 답변을 제공해주세요.\n",
    "\n",
    "참고할 테스트 스텝들:\n",
    "{context_str}\n",
    "\n",
    "사용자 질문: {query_str}\n",
    "\n",
    "답변 가이드라인:\n",
    "1. 관련된 테스트 스텝들을 명확하게 설명해주세요\n",
    "2. 각 테스트 스텝의 목적과 예상 결과를 포함해주세요\n",
    "3. 테스트 데이터가 있다면 함께 제시해주세요\n",
    "4. 이슈 키별로 그룹핑하여 전체적인 테스트 흐름을 보여주세요\n",
    "5. 찾은 정보가 충분하지 않다면 추가로 필요한 정보를 제안해주세요\n",
    "6. 답변은 한국어로 제공해주세요\n",
    "\n",
    "답변:\"\"\"\n",
    "        )\n",
    "        self.query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_template})\n",
    "    \n",
    "    def search_test_cases(self, \n",
    "                         query: str, \n",
    "                         filters: Optional[Dict[str, Any]] = None,\n",
    "                         top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            formatted_query = query \n",
    "            llama_filters = None\n",
    "            if filters:\n",
    "                llama_filters = MetadataFilters(\n",
    "                    filters=[ExactMatchFilter(key=k, value=v) for k, v in filters.items()],\n",
    "                    condition=FilterOperator.AND\n",
    "                )\n",
    "\n",
    "            retriever = VectorIndexRetriever(\n",
    "                index=self.index,\n",
    "                similarity_top_k=top_k,\n",
    "                filters=llama_filters\n",
    "            )\n",
    "            \n",
    "            nodes = retriever.retrieve(formatted_query)\n",
    "            \n",
    "            results = []\n",
    "            for node in nodes:\n",
    "                result = {\n",
    "                    \"content\": node.text,\n",
    "                    \"metadata\": node.metadata,\n",
    "                    \"similarity_score\": float(node.score) if node.score else 0.0,\n",
    "                    \"node_id\": node.node_id\n",
    "                }\n",
    "                results.append(result)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 검색 중 오류 발생: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def ask_question(self, question: str, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        try:\n",
    "            print(f\"🤖 LM Studio로 질문 처리 중: {question}\")\n",
    "            formatted_question = question\n",
    "            current_query_engine = self.query_engine\n",
    "            \n",
    "            if filters:\n",
    "                llama_filters = MetadataFilters(\n",
    "                    filters=[ExactMatchFilter(key=k, value=v) for k, v in filters.items()],\n",
    "                    condition=FilterOperator.AND\n",
    "                )\n",
    "\n",
    "                retriever = VectorIndexRetriever(\n",
    "                    index=self.index,\n",
    "                    similarity_top_k=8,\n",
    "                    filters=llama_filters\n",
    "                )\n",
    "                postprocessors = [SimilarityPostprocessor(similarity_cutoff=0.7)]\n",
    "                query_engine = RetrieverQueryEngine(\n",
    "                    retriever=retriever,\n",
    "                    node_postprocessors=postprocessors\n",
    "                )\n",
    "                qa_prompt_template = PromptTemplate(\n",
    "                    \"\"\"당신은 테스트케이스 전문가입니다. 주어진 테스트 스텝들을 바탕으로 사용자의 질문에 정확하고 유용한 답변을 제공해주세요.\n",
    "\n",
    "참고할 테스트 스텝들:\n",
    "{context_str}\n",
    "\n",
    "사용자 질문: {query_str}\n",
    "\n",
    "답변은 한국어로 제공해주세요.\n",
    "\n",
    "답변:\"\"\"\n",
    "                )\n",
    "                query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_template})\n",
    "                current_query_engine = query_engine\n",
    "            \n",
    "            response = current_query_engine.query(formatted_question)\n",
    "            \n",
    "            source_nodes = []\n",
    "            if hasattr(response, 'source_nodes'):\n",
    "                for node in response.source_nodes:\n",
    "                    source_nodes.append({\n",
    "                        \"content\": node.text,\n",
    "                        \"metadata\": node.metadata,\n",
    "                        \"score\": float(node.score) if node.score else 0.0\n",
    "                    })\n",
    "            \n",
    "            grouped_sources = self.group_results_by_issue(source_nodes)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": str(response),\n",
    "                \"source_nodes\": source_nodes,\n",
    "                \"grouped_sources\": grouped_sources,\n",
    "                \"response_metadata\": response.metadata if hasattr(response, 'metadata') else {}\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 질문 처리 중 오류 발생: {e}\")\n",
    "            return {\n",
    "                \"answer\": \"죄송합니다. 질문을 처리하는 중 오류가 발생했습니다.\",\n",
    "                \"source_nodes\": [],\n",
    "                \"grouped_sources\": {},\n",
    "                \"response_metadata\": {}\n",
    "            }\n",
    "    \n",
    "    def group_results_by_issue(self, results: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        grouped = {}\n",
    "        for result in results:\n",
    "            issue_key = result[\"metadata\"].get(\"issue_key\", \"UNKNOWN\")\n",
    "            if issue_key not in grouped:\n",
    "                grouped[issue_key] = []\n",
    "            grouped[issue_key].append(result)\n",
    "        \n",
    "        for issue_key in grouped:\n",
    "            grouped[issue_key].sort(\n",
    "                key=lambda x: int(x[\"metadata\"].get(\"step_index\", \"0\"))\n",
    "            )\n",
    "        return grouped\n",
    "    \n",
    "    def get_test_case_by_issue(self, issue_key: str) -> List[Dict[str, Any]]:\n",
    "        filters = {\"issue_key\": issue_key}\n",
    "        results = self.search_test_cases(\"\", filters=filters, top_k=50) \n",
    "        results.sort(key=lambda x: int(x[\"metadata\"].get(\"step_index\", \"0\")))\n",
    "        return results\n",
    "    \n",
    "    def create_custom_query_engine(self, \n",
    "                                  similarity_top_k: int = 8,\n",
    "                                  similarity_cutoff: float = 0.7,\n",
    "                                  filters: Optional[Dict[str, Any]] = None) -> RetrieverQueryEngine:\n",
    "        llama_filters = None\n",
    "        if filters:\n",
    "            llama_filters = MetadataFilters(\n",
    "                filters=[ExactMatchFilter(key=k, value=v) for k, v in filters.items()],\n",
    "                condition=FilterOperator.AND\n",
    "            )\n",
    "\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=self.index,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            filters=llama_filters\n",
    "        )\n",
    "        \n",
    "        postprocessors = [SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)]\n",
    "        \n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=postprocessors\n",
    "        )\n",
    "        \n",
    "        qa_prompt_template = PromptTemplate(\n",
    "            \"\"\"당신은 테스트케이스 전문가입니다. 주어진 테스트 스텝들을 바탕으로 사용자의 질문에 답변해주세요.\n",
    "\n",
    "참고할 테스트 스텝들:\n",
    "{context_str}\n",
    "\n",
    "사용자 질문: {query_str}\n",
    "\n",
    "답변은 한국어로 제공해주세요.\n",
    "\n",
    "답변:\"\"\"\n",
    "        )\n",
    "        query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_template})\n",
    "        return query_engine\n",
    "    \n",
    "    def test_lm_studio_direct(self, prompt: str) -> str:\n",
    "        response = Settings.llm.complete(prompt)\n",
    "        return response.text\n",
    "    \n",
    "    def print_search_results(self, results: List[Dict[str, Any]]):\n",
    "        if not results:\n",
    "            print(\"검색 결과가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        grouped = self.group_results_by_issue(results)\n",
    "        \n",
    "        for issue_key, steps in grouped.items():\n",
    "            print(f\"\\n🔍 이슈: {issue_key}\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for step in steps:\n",
    "                step_idx = step[\"metadata\"].get(\"step_index\", \"N/A\")\n",
    "                score = step.get(\"similarity_score\", 0)\n",
    "                print(f\"\\n📋 Step {step_idx} (유사도: {score:.3f})\")\n",
    "                print(\"-\" * 30)\n",
    "                print(step[\"content\"])\n",
    "    \n",
    "    def print_qa_result(self, result: Dict[str, Any]):\n",
    "        print(f\"\\n💡 LM Studio 답변:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(result[\"answer\"])\n",
    "        \n",
    "        if result[\"grouped_sources\"]:\n",
    "            print(f\"\\n📚 참고한 테스트케이스:\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for issue_key, steps in result[\"grouped_sources\"].items():\n",
    "                print(f\"\\n🔗 {issue_key}\")\n",
    "                for step in steps:\n",
    "                    step_idx = step[\"metadata\"].get(\"step_index\", \"N/A\")\n",
    "                    score = step.get(\"score\", 0)\n",
    "                    print(f\"  Step {step_idx} (점수: {score:.3f}): {step['content'][:100]}...\")\n",
    "\n",
    "    def save_json_to_file(self, filename: str, data: Dict[str, Any]):\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ 모든 결과가 '{filename}' 파일에 JSON 형식으로 저장되었습니다.\")\n",
    "        except IOError as e:\n",
    "            print(f\"❌ JSON 파일 저장 중 오류 발생: {e}\")\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    rag = TestCaseRAGLlamaIndexLMStudio(\n",
    "        lm_studio_url=\"http://127.0.0.1:1234/v1\",\n",
    "        lm_studio_model=\"qwen/qwen3-8b\"\n",
    "    )\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # LangChain 연동 예시\n",
    "    # ---------------------------------------------\n",
    "    print(\"\\n--- LangChain 연동 테스트 ---\")\n",
    "\n",
    "    # LM Studio를 LangChain의 LLM으로 사용하기 위해 OpenAIWrapper 설정\n",
    "    # LM Studio는 OpenAI API와 호환되므로 langchain_community.llms.OpenAI 사용\n",
    "    lc_llm = OpenAI(\n",
    "        base_url=rag.lm_studio_url,\n",
    "        model_name=rag.lm_studio_model,\n",
    "        temperature=rag.temperature,\n",
    "        max_tokens=rag.max_tokens\n",
    "    )\n",
    "    print(f\"✅ LangChain용 LM Studio LLM 설정 완료: {rag.lm_studio_url}\")\n",
    "\n",
    "    # LlamaIndex의 Retriever를 LangChain의 Retriever로 변환\n",
    "    # LlamaIndex의 기본 Retriever 설정 (필터링 없이)\n",
    "    llama_index_base_retriever = VectorIndexRetriever(\n",
    "        index=rag.index,\n",
    "        similarity_top_k=8,\n",
    "        vector_store_query_mode=\"default\"\n",
    "    )\n",
    "    # LlamaIndex Retriever를 LangChain Retriever로 래핑\n",
    "    lc_retriever = LlamaIndexRetriever(llama_index_base_retriever)\n",
    "    print(\"✅ LlamaIndex Retriever를 LangChain 호환 Retriever로 변환 완료\")\n",
    "\n",
    "    # LangChain RAG 프롬프트 템플릿 정의\n",
    "    # (LlamaIndex 프롬프트와 비슷하지만 LangChain PromptTemplate 사용)\n",
    "    lc_template = \"\"\"당신은 테스트케이스 전문가입니다. 주어진 테스트 스텝들을 바탕으로 사용자의 질문에 정확하고 유용한 답변을 제공해주세요.\n",
    "\n",
    "참고할 테스트 스텝들:\n",
    "{context}\n",
    "\n",
    "사용자 질문: {question}\n",
    "\n",
    "답변 가이드라인:\n",
    "1. 관련된 테스트 스텝들을 명확하게 설명해주세요\n",
    "2. 각 테스트 스텝의 목적과 예상 결과를 포함해주세요\n",
    "3. 테스트 데이터가 있다면 함께 제시해주세요\n",
    "4. 이슈 키별로 그룹핑하여 전체적인 테스트 흐름을 보여주세요\n",
    "5. 찾은 정보가 충분하지 않다면 추가로 필요한 정보를 제안해주세요\n",
    "6. 답변은 한국어로 제공해주세요\n",
    "\n",
    "답변:\"\"\"\n",
    "    lc_prompt = LCPromptTemplate.from_template(lc_template)\n",
    "\n",
    "    # LangChain RAG 체인 구축 (LCEL: LangChain Expression Language)\n",
    "    # RAG 체인은 retriever로 문서를 찾고, 이 문서를 context로 사용하여 LLM이 답변을 생성하도록 합니다.\n",
    "    lc_rag_chain = (\n",
    "        {\"context\": lc_retriever, \"question\": RunnablePassthrough()}\n",
    "        | lc_prompt\n",
    "        | lc_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"✅ LangChain RAG 체인 구축 완료\")\n",
    "\n",
    "    # LangChain RAG 체인을 사용하여 질문하기\n",
    "    langchain_question = \"master admin 테스트의 주요 시나리오는 무엇인가요?\"\n",
    "    print(f\"\\n💬 LangChain RAG 체인으로 질문 처리 중: {langchain_question}\")\n",
    "    langchain_answer = lc_rag_chain.invoke(langchain_question)\n",
    "    print(f\"\\n💡 LangChain RAG 답변:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(langchain_answer)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 기존 테스트 코드 (JSON 저장을 위해 all_test_results에 추가)\n",
    "    # ---------------------------------------------\n",
    "    all_test_results = {}\n",
    "\n",
    "    print(\"🤖 LM Studio 직접 테스트\")\n",
    "    direct_response = rag.test_lm_studio_direct(\"안녕하세요! 테스트 메시지입니다.\")\n",
    "    print(f\"응답: {direct_response}\")\n",
    "    all_test_results[\"direct_lm_studio_test\"] = {\"query\": \"안녕하세요! 테스트 메시지입니다.\", \"response\": direct_response}\n",
    "    \n",
    "    print(\"\\n🔍 기본 검색 테스트\")\n",
    "    search_query_1 = \"master admin 테스트 케이스\"\n",
    "    search_results_1 = rag.search_test_cases(search_query_1, top_k=5)\n",
    "    rag.print_search_results(search_results_1)\n",
    "    all_test_results[\"basic_search_test\"] = {\"query\": search_query_1, \"results\": search_results_1}\n",
    "    \n",
    "    print(\"\\n🔍 필터링 검색 테스트\")\n",
    "    search_query_2 = \"테스트\"\n",
    "    filters_2 = {\"issue_key\": \"COMMONR-380\"}\n",
    "    filtered_results_2 = rag.search_test_cases(search_query_2, filters=filters_2, top_k=5)\n",
    "    rag.print_search_results(filtered_results_2)\n",
    "    all_test_results[\"filtered_search_test\"] = {\"query\": search_query_2, \"filters\": filters_2, \"results\": filtered_results_2}\n",
    "    \n",
    "    print(\"\\n💬 LM Studio 질문 답변 테스트\")\n",
    "    qa_question_1 = \"master admin 테스트는 어떻게 해야 하나요?\"\n",
    "    qa_result_1 = rag.ask_question(qa_question_1)\n",
    "    rag.print_qa_result(qa_result_1)\n",
    "    all_test_results[\"qa_test_1\"] = {\"question\": qa_question_1, \"response\": qa_result_1}\n",
    "    \n",
    "    print(\"\\n🔧 커스텀 쿼리 엔진 테스트\")\n",
    "    custom_engine_query = \"master-slave master admin 테스트 방법\"\n",
    "    custom_engine_filters = {\"issue_key\": \"COMMONR-380\"} \n",
    "    custom_engine = rag.create_custom_query_engine(\n",
    "        similarity_top_k=5,\n",
    "        similarity_cutoff=0.8,\n",
    "        filters=custom_engine_filters \n",
    "    )\n",
    "    custom_response_obj = custom_engine.query(custom_engine_query)\n",
    "    custom_response_text = str(custom_response_obj)\n",
    "    print(f\"커스텀 답변: {custom_response_text}\")\n",
    "\n",
    "    # custom_engine의 source_nodes도 저장하려면, ask_question처럼 내부 로직을 수정하여\n",
    "    # source_nodes를 반환하도록 하거나, query_engine의 response 객체에서 직접 접근해야 합니다.\n",
    "    # 여기서는 간단히 최종 텍스트만 저장합니다.\n",
    "    all_test_results[\"custom_query_engine_test\"] = {\n",
    "        \"query\": custom_engine_query, \n",
    "        \"filters\": custom_engine_filters,\n",
    "        \"answer_text\": custom_response_text,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n💬 복잡한 질문 테스트\")\n",
    "    qa_question_2 = \"master admin 테스트에서 주의해야 할 점과 필수 테스트 시나리오를 알려주세요.\"\n",
    "    complex_qa_result = rag.ask_question(qa_question_2)\n",
    "    rag.print_qa_result(complex_qa_result)\n",
    "    all_test_results[\"complex_qa_test\"] = {\"question\": qa_question_2, \"response\": complex_qa_result}\n",
    "    \n",
    "    print(\"\\n📋 특정 이슈 전체 테스트 스텝 조회\")\n",
    "    issue_key_to_retrieve = \"COMMONR-380\"\n",
    "    issue_steps = rag.get_test_case_by_issue(issue_key_to_retrieve)\n",
    "    rag.print_search_results(issue_steps)\n",
    "    all_test_results[\"get_all_steps_by_issue\"] = {\"issue_key\": issue_key_to_retrieve, \"steps\": issue_steps}\n",
    "\n",
    "    # LangChain 결과도 JSON에 추가\n",
    "    all_test_results[\"langchain_rag_test\"] = {\n",
    "        \"question\": langchain_question,\n",
    "        \"answer\": langchain_answer\n",
    "    }\n",
    "\n",
    "    output_filename = \"rag_full_results.json\"\n",
    "    rag.save_json_to_file(output_filename, all_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Jira to 임베딩 형식 변환 ===\n",
      "Jira URL: https://jira.suprema.co.kr\n",
      "사용자: dhwoo\n",
      "JQL 쿼리: project = \"COMMONR\" AND issuetype = \"Test\"\n",
      "\n",
      "JQL로 이슈 검색을 시작합니다: project = \"COMMONR\" AND issuetype = \"Test\"\n",
      "  -> 현재까지 100 / 186 개 이슈 키를 가져왔습니다...\n",
      "  -> 현재까지 186 / 186 개 이슈 키를 가져왔습니다...\n",
      "총 186개의 이슈를 찾았습니다.\n",
      "\n",
      "이슈 데이터를 가져오고 임베딩 형식으로 변환하는 중...\n",
      "[1/186] COMMONR-380 처리 중...\n",
      " COMMONR-380 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[2/186] COMMONR-379 처리 중...\n",
      " COMMONR-379 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[3/186] COMMONR-378 처리 중...\n",
      " COMMONR-378 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[4/186] COMMONR-377 처리 중...\n",
      " COMMONR-377 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[5/186] COMMONR-376 처리 중...\n",
      " COMMONR-376 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[6/186] COMMONR-375 처리 중...\n",
      " COMMONR-375 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[7/186] COMMONR-374 처리 중...\n",
      " COMMONR-374 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[8/186] COMMONR-373 처리 중...\n",
      " COMMONR-373 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[9/186] COMMONR-372 처리 중...\n",
      " COMMONR-372 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[10/186] COMMONR-371 처리 중...\n",
      " COMMONR-371 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[11/186] COMMONR-370 처리 중...\n",
      " COMMONR-370 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[12/186] COMMONR-369 처리 중...\n",
      " COMMONR-369 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[13/186] COMMONR-368 처리 중...\n",
      " COMMONR-368 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[14/186] COMMONR-367 처리 중...\n",
      " COMMONR-367 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[15/186] COMMONR-366 처리 중...\n",
      " COMMONR-366 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[16/186] COMMONR-365 처리 중...\n",
      " COMMONR-365 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[17/186] COMMONR-363 처리 중...\n",
      " COMMONR-363 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[18/186] COMMONR-362 처리 중...\n",
      " COMMONR-362 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[19/186] COMMONR-361 처리 중...\n",
      " COMMONR-361 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[20/186] COMMONR-360 처리 중...\n",
      " COMMONR-360 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[21/186] COMMONR-359 처리 중...\n",
      "[22/186] COMMONR-356 처리 중...\n",
      " COMMONR-356 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[23/186] COMMONR-355 처리 중...\n",
      " COMMONR-355 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[24/186] COMMONR-354 처리 중...\n",
      " COMMONR-354 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[25/186] COMMONR-353 처리 중...\n",
      " COMMONR-353 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[26/186] COMMONR-352 처리 중...\n",
      " COMMONR-352 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[27/186] COMMONR-351 처리 중...\n",
      " COMMONR-351 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[28/186] COMMONR-350 처리 중...\n",
      " COMMONR-350 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[29/186] COMMONR-330 처리 중...\n",
      " COMMONR-330 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[30/186] COMMONR-329 처리 중...\n",
      " COMMONR-329 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[31/186] COMMONR-328 처리 중...\n",
      " COMMONR-328 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[32/186] COMMONR-327 처리 중...\n",
      " COMMONR-327 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[33/186] COMMONR-326 처리 중...\n",
      " COMMONR-326 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[34/186] COMMONR-325 처리 중...\n",
      " COMMONR-325 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[35/186] COMMONR-324 처리 중...\n",
      " COMMONR-324 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[36/186] COMMONR-322 처리 중...\n",
      " COMMONR-322 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[37/186] COMMONR-321 처리 중...\n",
      " COMMONR-321 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[38/186] COMMONR-306 처리 중...\n",
      " COMMONR-306 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[39/186] COMMONR-305 처리 중...\n",
      " COMMONR-305 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[40/186] COMMONR-304 처리 중...\n",
      " COMMONR-304 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[41/186] COMMONR-303 처리 중...\n",
      " COMMONR-303 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[42/186] COMMONR-302 처리 중...\n",
      " COMMONR-302 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[43/186] COMMONR-299 처리 중...\n",
      " COMMONR-299 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[44/186] COMMONR-298 처리 중...\n",
      " COMMONR-298 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[45/186] COMMONR-297 처리 중...\n",
      " COMMONR-297 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[46/186] COMMONR-296 처리 중...\n",
      " COMMONR-296 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[47/186] COMMONR-295 처리 중...\n",
      " COMMONR-295 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[48/186] COMMONR-294 처리 중...\n",
      " COMMONR-294 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[49/186] COMMONR-293 처리 중...\n",
      " COMMONR-293 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[50/186] COMMONR-292 처리 중...\n",
      " COMMONR-292 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[51/186] COMMONR-291 처리 중...\n",
      " COMMONR-291 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[52/186] COMMONR-290 처리 중...\n",
      " COMMONR-290 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[53/186] COMMONR-289 처리 중...\n",
      " COMMONR-289 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[54/186] COMMONR-288 처리 중...\n",
      " COMMONR-288 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[55/186] COMMONR-287 처리 중...\n",
      " COMMONR-287 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[56/186] COMMONR-286 처리 중...\n",
      " COMMONR-286 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[57/186] COMMONR-285 처리 중...\n",
      " COMMONR-285 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[58/186] COMMONR-284 처리 중...\n",
      " COMMONR-284 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[59/186] COMMONR-283 처리 중...\n",
      " COMMONR-283 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[60/186] COMMONR-280 처리 중...\n",
      " COMMONR-280 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[61/186] COMMONR-279 처리 중...\n",
      " COMMONR-279 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[62/186] COMMONR-278 처리 중...\n",
      " COMMONR-278 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[63/186] COMMONR-277 처리 중...\n",
      " COMMONR-277 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[64/186] COMMONR-276 처리 중...\n",
      " COMMONR-276 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[65/186] COMMONR-274 처리 중...\n",
      " COMMONR-274 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[66/186] COMMONR-272 처리 중...\n",
      " COMMONR-272 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[67/186] COMMONR-271 처리 중...\n",
      " COMMONR-271 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[68/186] COMMONR-270 처리 중...\n",
      " COMMONR-270 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[69/186] COMMONR-269 처리 중...\n",
      " COMMONR-269 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[70/186] COMMONR-268 처리 중...\n",
      " COMMONR-268 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[71/186] COMMONR-267 처리 중...\n",
      " COMMONR-267 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[72/186] COMMONR-265 처리 중...\n",
      " COMMONR-265 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[73/186] COMMONR-264 처리 중...\n",
      " COMMONR-264 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[74/186] COMMONR-263 처리 중...\n",
      " COMMONR-263 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[75/186] COMMONR-262 처리 중...\n",
      " COMMONR-262 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[76/186] COMMONR-261 처리 중...\n",
      " COMMONR-261 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[77/186] COMMONR-260 처리 중...\n",
      " COMMONR-260 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[78/186] COMMONR-259 처리 중...\n",
      " COMMONR-259 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[79/186] COMMONR-258 처리 중...\n",
      " COMMONR-258 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[80/186] COMMONR-256 처리 중...\n",
      " COMMONR-256 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[81/186] COMMONR-255 처리 중...\n",
      " COMMONR-255 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[82/186] COMMONR-254 처리 중...\n",
      " COMMONR-254 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[83/186] COMMONR-253 처리 중...\n",
      " COMMONR-253 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[84/186] COMMONR-252 처리 중...\n",
      " COMMONR-252 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[85/186] COMMONR-251 처리 중...\n",
      " COMMONR-251 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[86/186] COMMONR-250 처리 중...\n",
      " COMMONR-250 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[87/186] COMMONR-249 처리 중...\n",
      " COMMONR-249 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[88/186] COMMONR-248 처리 중...\n",
      " COMMONR-248 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[89/186] COMMONR-247 처리 중...\n",
      " COMMONR-247 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[90/186] COMMONR-245 처리 중...\n",
      " COMMONR-245 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[91/186] COMMONR-244 처리 중...\n",
      " COMMONR-244 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[92/186] COMMONR-243 처리 중...\n",
      " COMMONR-243 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[93/186] COMMONR-242 처리 중...\n",
      " COMMONR-242 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[94/186] COMMONR-241 처리 중...\n",
      " COMMONR-241 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[95/186] COMMONR-240 처리 중...\n",
      " COMMONR-240 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[96/186] COMMONR-239 처리 중...\n",
      " COMMONR-239 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[97/186] COMMONR-238 처리 중...\n",
      " COMMONR-238 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[98/186] COMMONR-237 처리 중...\n",
      " COMMONR-237 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[99/186] COMMONR-235 처리 중...\n",
      " COMMONR-235 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[100/186] COMMONR-234 처리 중...\n",
      " COMMONR-234 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[101/186] COMMONR-231 처리 중...\n",
      " COMMONR-231 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[102/186] COMMONR-226 처리 중...\n",
      " COMMONR-226 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[103/186] COMMONR-224 처리 중...\n",
      " COMMONR-224 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[104/186] COMMONR-221 처리 중...\n",
      " COMMONR-221 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[105/186] COMMONR-220 처리 중...\n",
      " COMMONR-220 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[106/186] COMMONR-218 처리 중...\n",
      " COMMONR-218 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[107/186] COMMONR-217 처리 중...\n",
      " COMMONR-217 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[108/186] COMMONR-216 처리 중...\n",
      " COMMONR-216 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[109/186] COMMONR-211 처리 중...\n",
      " COMMONR-211 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[110/186] COMMONR-210 처리 중...\n",
      " COMMONR-210 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[111/186] COMMONR-209 처리 중...\n",
      " COMMONR-209 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[112/186] COMMONR-208 처리 중...\n",
      " COMMONR-208 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[113/186] COMMONR-207 처리 중...\n",
      " COMMONR-207 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[114/186] COMMONR-200 처리 중...\n",
      " COMMONR-200 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[115/186] COMMONR-199 처리 중...\n",
      " COMMONR-199 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[116/186] COMMONR-198 처리 중...\n",
      " COMMONR-198 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[117/186] COMMONR-195 처리 중...\n",
      " COMMONR-195 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[118/186] COMMONR-193 처리 중...\n",
      " COMMONR-193 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[119/186] COMMONR-192 처리 중...\n",
      " COMMONR-192 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[120/186] COMMONR-179 처리 중...\n",
      " COMMONR-179 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[121/186] COMMONR-178 처리 중...\n",
      " COMMONR-178 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[122/186] COMMONR-177 처리 중...\n",
      " COMMONR-177 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[123/186] COMMONR-176 처리 중...\n",
      " COMMONR-176 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[124/186] COMMONR-167 처리 중...\n",
      "[125/186] COMMONR-156 처리 중...\n",
      " COMMONR-156 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[126/186] COMMONR-151 처리 중...\n",
      " COMMONR-151 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[127/186] COMMONR-149 처리 중...\n",
      " COMMONR-149 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[128/186] COMMONR-148 처리 중...\n",
      " COMMONR-148 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[129/186] COMMONR-126 처리 중...\n",
      " COMMONR-126 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[130/186] COMMONR-124 처리 중...\n",
      " COMMONR-124 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[131/186] COMMONR-123 처리 중...\n",
      " COMMONR-123 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[132/186] COMMONR-107 처리 중...\n",
      " COMMONR-107 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[133/186] COMMONR-106 처리 중...\n",
      " COMMONR-106 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[134/186] COMMONR-104 처리 중...\n",
      " COMMONR-104 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[135/186] COMMONR-103 처리 중...\n",
      " COMMONR-103 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[136/186] COMMONR-101 처리 중...\n",
      " COMMONR-101 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[137/186] COMMONR-100 처리 중...\n",
      " COMMONR-100 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[138/186] COMMONR-99 처리 중...\n",
      " COMMONR-99 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[139/186] COMMONR-95 처리 중...\n",
      " COMMONR-95 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[140/186] COMMONR-93 처리 중...\n",
      " COMMONR-93 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[141/186] COMMONR-82 처리 중...\n",
      " COMMONR-82 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[142/186] COMMONR-79 처리 중...\n",
      " COMMONR-79 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[143/186] COMMONR-78 처리 중...\n",
      " COMMONR-78 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[144/186] COMMONR-77 처리 중...\n",
      " COMMONR-77 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[145/186] COMMONR-76 처리 중...\n",
      " COMMONR-76 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[146/186] COMMONR-75 처리 중...\n",
      " COMMONR-75 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[147/186] COMMONR-74 처리 중...\n",
      " COMMONR-74 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[148/186] COMMONR-72 처리 중...\n",
      " COMMONR-72 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[149/186] COMMONR-71 처리 중...\n",
      " COMMONR-71 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[150/186] COMMONR-70 처리 중...\n",
      " COMMONR-70 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[151/186] COMMONR-69 처리 중...\n",
      " COMMONR-69 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[152/186] COMMONR-68 처리 중...\n",
      " COMMONR-68 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[153/186] COMMONR-62 처리 중...\n",
      " COMMONR-62 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[154/186] COMMONR-60 처리 중...\n",
      " COMMONR-60 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[155/186] COMMONR-58 처리 중...\n",
      " COMMONR-58 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[156/186] COMMONR-57 처리 중...\n",
      " COMMONR-57 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[157/186] COMMONR-56 처리 중...\n",
      " COMMONR-56 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[158/186] COMMONR-55 처리 중...\n",
      " COMMONR-55 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[159/186] COMMONR-54 처리 중...\n",
      " COMMONR-54 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[160/186] COMMONR-44 처리 중...\n",
      " COMMONR-44 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[161/186] COMMONR-42 처리 중...\n",
      " COMMONR-42 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[162/186] COMMONR-41 처리 중...\n",
      " COMMONR-41 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[163/186] COMMONR-40 처리 중...\n",
      " COMMONR-40 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[164/186] COMMONR-39 처리 중...\n",
      " COMMONR-39 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[165/186] COMMONR-38 처리 중...\n",
      " COMMONR-38 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[166/186] COMMONR-36 처리 중...\n",
      " COMMONR-36 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[167/186] COMMONR-35 처리 중...\n",
      " COMMONR-35 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[168/186] COMMONR-34 처리 중...\n",
      " COMMONR-34 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[169/186] COMMONR-32 처리 중...\n",
      " COMMONR-32 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[170/186] COMMONR-31 처리 중...\n",
      " COMMONR-31 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[171/186] COMMONR-30 처리 중...\n",
      " COMMONR-30 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[172/186] COMMONR-29 처리 중...\n",
      " COMMONR-29 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[173/186] COMMONR-24 처리 중...\n",
      " COMMONR-24 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[174/186] COMMONR-23 처리 중...\n",
      " COMMONR-23 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[175/186] COMMONR-22 처리 중...\n",
      " COMMONR-22 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[176/186] COMMONR-21 처리 중...\n",
      " COMMONR-21 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[177/186] COMMONR-20 처리 중...\n",
      " COMMONR-20 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[178/186] COMMONR-19 처리 중...\n",
      " COMMONR-19 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[179/186] COMMONR-15 처리 중...\n",
      " COMMONR-15 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[180/186] COMMONR-13 처리 중...\n",
      " COMMONR-13 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[181/186] COMMONR-12 처리 중...\n",
      " COMMONR-12 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[182/186] COMMONR-11 처리 중...\n",
      " COMMONR-11 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[183/186] COMMONR-9 처리 중...\n",
      " COMMONR-9 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[184/186] COMMONR-8 처리 중...\n",
      " COMMONR-8 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[185/186] COMMONR-4 처리 중...\n",
      " COMMONR-4 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "[186/186] COMMONR-3 처리 중...\n",
      " COMMONR-3 이슈의 정보를 json으로 저장을 시작합니다.\n",
      "저장 폴더: 'c:\\Users\\dhwoo\\Desktop\\project\\RAG\\jira_issues_output'\n",
      "\n",
      "성공적으로 처리된 이슈: 184/186\n",
      "\n",
      "=== 임베딩 데이터 샘플 ===\n",
      "샘플 1:\n",
      "ID: COMMONR-380_step_1\n",
      "Document: Test Step: 1. Device> 관리자 설정\n",
      "Test Data: 1. Master Password 설정\n",
      "2. 전체 관리자 설정\n",
      "3. 장치 설정 관리자 설정\n",
      "4. 사용자 관리...\n",
      "Metadata: {'issue_key': 'COMMONR-380', 'step_index': '1', 'source': 'jira_test_step'}\n",
      "--------------------------------------------------\n",
      "샘플 2:\n",
      "ID: COMMONR-380_step_2\n",
      "Document: Test Step: [전체 관리자 미설정 상태]\n",
      "1. 장치 내에서 사용자에 사용자관리자 & 설정 관리자 설정\n",
      "2. BioStar X에서 장치>고급에서 사용자관리자 & 설정 관리자 ...\n",
      "Metadata: {'issue_key': 'COMMONR-380', 'step_index': '2', 'source': 'jira_test_step'}\n",
      "--------------------------------------------------\n",
      "샘플 3:\n",
      "ID: COMMONR-380_step_3\n",
      "Document: Test Step: 1. Device> 전체관리자 설정\n",
      "2. 사용자 관리자 & 설정관리자 설정\n",
      "3. 설정한 관리자로 출입문 제어 진입 시도\n",
      "Expected Result: 1. 인증...\n",
      "Metadata: {'issue_key': 'COMMONR-380', 'step_index': '3', 'source': 'jira_test_step'}\n",
      "--------------------------------------------------\n",
      "\n",
      "=== 통계 ===\n",
      "총 2632개의 임베딩 객체 생성\n",
      "이슈당 평균 14.2개의 테스트 스텝\n",
      "평균 문서 길이: 310 문자\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    print(\"=== Jira to 임베딩 형식 변환 ===\")\n",
    "    print(f\"Jira URL: {JIRA_URL}\")\n",
    "    print(f\"사용자: {JIRA_USERNAME}\")\n",
    "    print(f\"JQL 쿼리: {JQL_QUERY}\")\n",
    "    print()\n",
    "\n",
    "    # 1. 이슈 키 목록 가져오기\n",
    "    issue_keys = get_all_jira_issue_keys(JQL_QUERY)\n",
    "    \n",
    "    if not issue_keys:\n",
    "        print(\"검색된 이슈가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"총 {len(issue_keys)}개의 이슈를 찾았습니다.\")\n",
    "    print()\n",
    "    \n",
    "    # 2. 모든 이슈 데이터를 임베딩 형태로 변환\n",
    "    all_embedding_data = []\n",
    "    successful_count = 0\n",
    "    \n",
    "    print(\"이슈 데이터를 가져오고 임베딩 형식으로 변환하는 중...\")\n",
    "    \n",
    "    for i, issue_key in enumerate(issue_keys, 1):\n",
    "        print(f\"[{i}/{len(issue_keys)}] {issue_key} 처리 중...\")\n",
    "        \n",
    "        # 임베딩용 형태로 변환\n",
    "        embedding_objects = convert_to_embedding_format(issue_key)\n",
    "        if embedding_objects:\n",
    "            all_embedding_data.extend(embedding_objects)  # 리스트 확장\n",
    "            # 개별 이슈별로 JSON 파일 저장\n",
    "            json_to_save_file(embedding_objects, issue_key)\n",
    "            successful_count += 1\n",
    "        \n",
    "        # API 호출 제한 고려\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(f\"\\n성공적으로 처리된 이슈: {successful_count}/{len(issue_keys)}\")\n",
    "    \n",
    "    # 3. 임베딩 데이터 출력 (처음 3개만 샘플로)\n",
    "    print(\"\\n=== 임베딩 데이터 샘플 ===\")\n",
    "    for i, item in enumerate(all_embedding_data[:3]):\n",
    "        print(f\"샘플 {i+1}:\")\n",
    "        print(f\"ID: {item['id']}\")\n",
    "        print(f\"Document: {item['document'][:100]}...\")\n",
    "        print(f\"Metadata: {item['metadata']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # 4. 통계\n",
    "    print(f\"\\n=== 통계 ===\")\n",
    "    print(f\"총 {len(all_embedding_data)}개의 임베딩 객체 생성\")\n",
    "    print(f\"이슈당 평균 {len(all_embedding_data)/len(issue_keys):.1f}개의 테스트 스텝\")\n",
    "    \n",
    "    # 평균 문서 길이\n",
    "    if all_embedding_data:\n",
    "        avg_doc_length = sum(len(item['document']) for item in all_embedding_data) / len(all_embedding_data)\n",
    "        print(f\"평균 문서 길이: {avg_doc_length:.0f} 문자\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"스크립트 실행 중...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
